[{"content":"Introduction I bought, \u0026ldquo;Build It with Phoenix\u0026rdquo; by Geoffrey Lessel on 2023-08-2023 on sale.\nI didn\u0026rsquo;t start on this until 2024-11-17 and finished it 2024-12-04.\nIt\u0026rsquo;s a program with videos and github codes to teach Phoenix and liveview.\nOutline Pros Teaches and goes into the mindset of how to debug How to read the document How Phoenix Auth generator works and how to use it How to upkeep a Phoenix project (Elixir in general too) Cons Not in depth about liveview and Phoenix. A tad outdated still workable (phx-update=\u0026quot;append\u0026quot;) Pros Teaches and goes into the mindset of how to debug iex ecto debug with iex how to read error message and track down the line of codes that\u0026rsquo;s causing it IO.inspect How to upkeep a Phoenix project (Elixir in general too) mix hex.outdated mix hex.clean --all mix hex.update --all mix clean mix compile Cons It\u0026rsquo;s not as in depth as, \u0026ldquo;Real-Time Phoenix\u0026rdquo; by Stephen Bussey.\nConclusion Overall, a great introduction to Phoenix. I have a decent amount of experience of Phoenix, but I learned quite a bit.\nThe biggest thing I like was how to think about context.\nIt gave me a gentle introduction to liveview.\nIt\u0026rsquo;s a good first step into Phoenix and liveview and would recommend it.\nIt\u0026rsquo;s a bit tad expensive though at $129.\nI hope Geo will update it with liveview version 1.0.\n","date":"2024-12-05T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/build-it-with-phoenix-review/biwp_hu9a73acadf11f7bc00f8390d3762fe581_64040_120x120_fill_box_smart1_3.png","permalink":"https://mythicalprogrammer.github.io/p/build-it-with-phoenix-review/","title":"Build It with Phoenix Review"},{"content":"Introduction There isn\u0026rsquo;t many resources on how to do view in the Phoenix framework (1.7.14) I read the official document and searched the web and it was lacking.\nEspecially on passing data onto the view template from component.\nPhoenix framework (1.7.14) is MVC but have changed the way V(view) works. So this is just a documentation for myself.\nFile Structure ./gira/*\n1 2 3 4 5 6 7 8 9 10 assets/ config/ lib/ priv/ test/ .formatter.exs .gitignore README.md mix.exs mix.lock ./gira/lib/gira_web/\n1 2 3 4 5 6 components/ controllers/ endpoint.ex gettext.ex router.ex telemetry.ex ./gira/lib/gira_web/controllers/\nNotice the page_controller.ex, page_html.ex, and page_html/.\nThe page_html.ex and page_html/ is where the \u0026ldquo;view\u0026rdquo; logics are located. This is the convention in the Phoenix framework (1.7.14) and I would suggest sticking to the default.\nAgain controllers and views lives in the controllers/ folder. View in the Phoenix framework (1.7.14) is postscript with _html and are further organized in two places.\n1 2 3 4 5 page_html/ error_html.ex error_json.ex page_controller.ex page_html.ex ./gira/lib/gira_web/components/\n1 2 3 layouts/ core_components.ex layouts.ex Codes - Simple Component for layout This is a simple component navbar that doesn\u0026rsquo;t uses any data (at least not yet). I decided to put the navbar view logic in the layout.\n./gira/lib/gira_web/components/layouts.ex\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 defmodule GiraWeb.Layouts do @moduledoc \u0026#34;\u0026#34;\u0026#34; This module holds different layouts used by your application. See the `layouts` directory for all templates available. The \u0026#34;root\u0026#34; layout is a skeleton rendered as part of the application router. The \u0026#34;app\u0026#34; layout is set as the default layout on both `use GiraWeb, :controller` and `use GiraWeb, :live_view`. \u0026#34;\u0026#34;\u0026#34; use GiraWeb, :html embed_templates \u0026#34;layouts/*\u0026#34; def navbar(assigns) do ~H\u0026#34;\u0026#34;\u0026#34; \u0026lt;nav class=\u0026#34;bg-white dark:bg-gray-900 fixed w-full z-20 top-0 start-0 border-b border-gray-200 dark:border-gray-600\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;max-w-screen-xl flex flex-wrap items-center justify-between mx-auto p-4\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;flex items-center space-x-3 rtl:space-x-reverse\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;https://flowbite.com/docs/images/logo.svg\u0026#34; class=\u0026#34;h-8\u0026#34; alt=\u0026#34;Flowbite Logo\u0026#34; /\u0026gt; \u0026lt;span class=\u0026#34;self-center text-2xl font-semibold whitespace-nowrap dark:text-white\u0026#34;\u0026gt;GiraUpdates\u0026lt;/span\u0026gt; \u0026lt;/a\u0026gt; \u0026lt;button data-collapse-toggle=\u0026#34;navbar-dropdown\u0026#34; type=\u0026#34;button\u0026#34; class=\u0026#34;inline-flex items-center p-2 w-10 h-10 justify-center text-sm text-gray-500 rounded-lg md:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600\u0026#34; aria-controls=\u0026#34;navbar-dropdown\u0026#34; aria-expanded=\u0026#34;false\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;sr-only\u0026#34;\u0026gt;Open main menu\u0026lt;/span\u0026gt; \u0026lt;svg class=\u0026#34;w-5 h-5\u0026#34; aria-hidden=\u0026#34;true\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; fill=\u0026#34;none\u0026#34; viewBox=\u0026#34;0 0 17 14\u0026#34;\u0026gt; \u0026lt;path stroke=\u0026#34;currentColor\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34; stroke-width=\u0026#34;2\u0026#34; d=\u0026#34;M1 1h15M1 7h15M1 13h15\u0026#34;/\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;div class=\u0026#34;hidden w-full md:block md:w-auto\u0026#34; id=\u0026#34;navbar-dropdown\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;flex flex-col font-medium p-4 md:p-0 mt-4 border border-gray-100 rounded-lg bg-gray-50 md:space-x-8 rtl:space-x-reverse md:flex-row md:mt-0 md:border-0 md:bg-white dark:bg-gray-800 md:dark:bg-gray-900 dark:border-gray-700\u0026#34;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block py-2 px-3 text-white bg-blue-700 rounded md:bg-transparent md:text-blue-700 md:p-0 md:dark:text-blue-500 dark:bg-blue-600 md:dark:bg-transparent\u0026#34; aria-current=\u0026#34;page\u0026#34;\u0026gt;Home\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;button id=\u0026#34;dropdownNavbarLink\u0026#34; data-dropdown-toggle=\u0026#34;dropdownNavbar\u0026#34; class=\u0026#34;flex items-center justify-between w-full py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 md:w-auto dark:text-white md:dark:hover:text-blue-500 dark:focus:text-white dark:border-gray-700 dark:hover:bg-gray-700 md:dark:hover:bg-transparent\u0026#34;\u0026gt;Reading\u0026lt;svg class=\u0026#34;w-2.5 h-2.5 ms-2.5\u0026#34; aria-hidden=\u0026#34;true\u0026#34; xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; fill=\u0026#34;none\u0026#34; viewBox=\u0026#34;0 0 10 6\u0026#34;\u0026gt; \u0026lt;path stroke=\u0026#34;currentColor\u0026#34; stroke-linecap=\u0026#34;round\u0026#34; stroke-linejoin=\u0026#34;round\u0026#34; stroke-width=\u0026#34;2\u0026#34; d=\u0026#34;m1 1 4 4 4-4\u0026#34;/\u0026gt; \u0026lt;/svg\u0026gt;\u0026lt;/button\u0026gt; \u0026lt;!-- Dropdown menu --\u0026gt; \u0026lt;div id=\u0026#34;dropdownNavbar\u0026#34; class=\u0026#34;z-10 hidden font-normal bg-white divide-y divide-gray-100 rounded-lg shadow w-44 dark:bg-gray-700 dark:divide-gray-600\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;py-2 text-sm text-gray-700 dark:text-gray-400\u0026#34; aria-labelledby=\u0026#34;dropdownLargeButton\u0026#34;\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block px-4 py-2 hover:bg-gray-100 dark:hover:bg-gray-600 dark:hover:text-white\u0026#34;\u0026gt;Dashboard\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block px-4 py-2 hover:bg-gray-100 dark:hover:bg-gray-600 dark:hover:text-white\u0026#34;\u0026gt;Settings\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block px-4 py-2 hover:bg-gray-100 dark:hover:bg-gray-600 dark:hover:text-white\u0026#34;\u0026gt;Earnings\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;div class=\u0026#34;py-1\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block px-4 py-2 text-sm text-gray-700 hover:bg-gray-100 dark:hover:bg-gray-600 dark:text-gray-200 dark:hover:text-white\u0026#34;\u0026gt;Sign out\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent\u0026#34;\u0026gt;Series\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent\u0026#34;\u0026gt;Tools\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;#\u0026#34; class=\u0026#34;block py-2 px-3 text-gray-900 rounded hover:bg-gray-100 md:hover:bg-transparent md:border-0 md:hover:text-blue-700 md:p-0 dark:text-white md:dark:hover:text-blue-500 dark:hover:bg-gray-700 dark:hover:text-white md:dark:hover:bg-transparent\u0026#34;\u0026gt;Login\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/nav\u0026gt; \u0026#34;\u0026#34;\u0026#34; end end ./gira/lib/gira_web/components/layouts/app.html.heex\n1 \u0026lt;.navbar /\u0026gt; Codes - Complex Component with Controller (connecting with model) This one is to have a table component, the rows are generated base on the data given by the model.\n./gira/lib/gira_web/controllers/page_controller.ex\n1 2 3 4 5 6 7 8 9 10 11 12 13 defmodule GiraWeb.PageController do use GiraWeb, :controller alias Gira.NovelUpdate #alias Gira.NovelUpdate.Update def home(conn, _params) do # The home page is often custom made, # so skip the default app layout. updates = NovelUpdate.list_updates() render(conn, :home, updates: updates) end end ./gira/lib/gira_web/controllers/page_html.ex\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 defmodule GiraWeb.PageHTML do @moduledoc \u0026#34;\u0026#34;\u0026#34; This module contains pages rendered by PageController. See the `page_html` directory for all templates available. \u0026#34;\u0026#34;\u0026#34; use GiraWeb, :html embed_templates \u0026#34;page_html/*\u0026#34; def table_update(assigns) do ~H\u0026#34;\u0026#34;\u0026#34; \u0026lt;div class=\u0026#34;relative overflow-x-auto shadow-md sm:rounded-lg\u0026#34;\u0026gt; \u0026lt;table class=\u0026#34;w-full text-sm text-left rtl:text-right text-gray-500 dark:text-gray-400\u0026#34;\u0026gt; \u0026lt;thead class=\u0026#34;text-xs text-gray-700 uppercase bg-gray-50 dark:bg-gray-700 dark:text-gray-400\u0026#34;\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34; class=\u0026#34;px-6 py-3\u0026#34;\u0026gt; Date \u0026lt;/th\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34; class=\u0026#34;px-6 py-3\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;flex items-center\u0026#34;\u0026gt; Title \u0026lt;/div\u0026gt; \u0026lt;/th\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34; class=\u0026#34;px-6 py-3\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;flex items-center\u0026#34;\u0026gt; Release \u0026lt;/div\u0026gt; \u0026lt;/th\u0026gt; \u0026lt;th scope=\u0026#34;col\u0026#34; class=\u0026#34;px-6 py-3\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;flex items-center\u0026#34;\u0026gt; Translation Group \u0026lt;/div\u0026gt; \u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; \u0026lt;tbody\u0026gt; \u0026lt;tr class=\u0026#34;odd:bg-white odd:dark:bg-gray-900 even:bg-gray-50 even:dark:bg-gray-800 border-b dark:border-gray-700\u0026#34; :for={update \u0026lt;- @updates}\u0026gt; \u0026lt;td class=\u0026#34;px-6 py-4\u0026#34;\u0026gt; \u0026lt;%= update.update_date %\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td scope=\u0026#34;row\u0026#34; class=\u0026#34;px-6 py-4 font-medium text-gray-900 whitespace-nowrap dark:text-white\u0026#34;\u0026gt; \u0026lt;%= update.title %\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;px-6 py-4\u0026#34;\u0026gt; \u0026lt;%= update.release %\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;td class=\u0026#34;px-6 py-4\u0026#34;\u0026gt; \u0026lt;%= update.translation_group %\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/tbody\u0026gt; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026#34;\u0026#34;\u0026#34; end end ./gira/lib/gira_web/controllers/page_html/home.html.heex\nAs of the writing this, I couldn\u0026rsquo;t find an explicit example of how to connect model data to the template and component. I\u0026rsquo;ve tried countless combination until I figured it out especially the code below.\n1 \u0026lt;.table_update updates={@updates} /\u0026gt; Credit Ai Generated Phoenix Fantasy royalty-free stock illustration. Free for use \u0026amp; download. ","date":"2024-07-22T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/phoenix-v1-7-14-views-components-and-how-it-is-different-from-other-mvc-frameworks/ai-generated-8579708_1280_hu3ed2371e628725aa6bc25e07c85826b2_263412_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/phoenix-v1-7-14-views-components-and-how-it-is-different-from-other-mvc-frameworks/","title":"Phoenix v1.7.14 Views (components) and how it is different from other MVC frameworks"},{"content":"Objective Just to document my set up so I can come back and refer to it. And for this to be newbie friendly.\nTo set up an online cloud environment to run Stable Diffusion to create AI video (with ComfyUI and AnimateDiff).\nIt would save me money via:\nElectricity Cost Graphic Card (which will depreciate overtime with new technology) Real life reason:\nI ran out of memory and it crash my queue when I was doing AnimateDiff. 6GB VRAM on my laptop wasn\u0026rsquo;t enough and it took over 16 hours to run a partial portion of it.\nCloud Provider https://www.runpod.io/\nI use Runpod because that\u0026rsquo;s the first option I\u0026rsquo;ve found.\nI also found a tutorial on setup via youtube: ComfyUI AnimateDiff Prompt Travel: Runpod.io Cloud GPUs Tutorial by c0nsumption\nSetting up a Pod (a GPU cloud instance) To start spinning up a new instance first click on the \u0026lsquo;New Pod\u0026rsquo; option over at the console.\nPod Console Pod Template Make sure Pod Template is set to: \u0026lsquo;RunPod SD Comfy UI\u0026rsquo;\nPod Cloud Type Select cloud type using the Cloud Type drop down filter menu. I chose the \u0026lsquo;Community Cloud\u0026rsquo; option because it\u0026rsquo;s cheaper.\nSelect GPU I chose this option for my first run.\nClick the option, \u0026lsquo;Customize Deployment\u0026rsquo;.\nAnd change Container Disk to 25GB. Some installation package will eat up more than 5GB (default).\nClick the option, \u0026lsquo;Deploy\u0026rsquo;.\nYou will end up with the next menu, check all the option is correct. Make sure the \u0026lsquo;Start Jupyter Notebook\u0026rsquo; checkbox is checked. Once confirming that the GPU instance configurations is correct click on the button, \u0026lsquo;Continue\u0026rsquo;.\nDeploy GPU Instance Look over the pricing summary for the extra/hidden cost on top of the GPU pricing rate. Once you\u0026rsquo;re okay with the total cost, click on the button \u0026lsquo;Deploy\u0026rsquo;.\nIt\u0026rsquo;ll redirect you to the Pod Console where your new instance is now created but not started.\nMore info on the pod by clicking the purple arrow button.\nPod Customization Set up ComfyUI port 8188 IMPORTANT\nClick on the hamburger button (the three horizontal line button) aka More Actions.\nSelect \u0026lsquo;Edit Pod\u0026rsquo;.\nYou should get this menu.\nFrom the menu above, locate \u0026lsquo;Expose TCP Ports\u0026rsquo;. Add 8188 to that field. Note you should now have \u0026lsquo;22,8188\u0026rsquo; (see picture below). Port 8188 is the default ComfyUI port. Click Save.\nPod ComfyUI Setup Connect to Juypter Click on the \u0026lsquo;Connect\u0026rsquo;.\nYou\u0026rsquo;ll get this menu:\nSelect the option Connect to Jupyter Lab [Port 8888]. This will open a new tab in your browser with Jupyter Lab.\nJupyter Lab Terminal From the Other menu, select the Terminal option.\nFrom here you should get this terminal:\nNOTE/WARNING: Anything in the /workspace directory will persist and exist after you stop your instance. Anything outside of this folder will not persist.\nSetup Python Virtual Environment Type below into Jupyter\u0026rsquo;s terminal:\n1 python -m venv venv Activate virtual environment:\n1 source venv/bin/activate Latest ComfyUI version Get the latest ComfyUI version.\n1 2 cd /workspace/ComfyUI git pull ComfyUI setup Nvidia Following the Linux Nvidia installation instruction.\nType the commands below in Jupyter Terminal:\n1 2 cd ComfyUI pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu121 Then do this:\n1 pip install -r requirements.txt ComfyUI Manager Installation Go to the custom_nodes folder:\n1 cd /workspace/ComfyUI/custom_nodes/ Once in the custom_nodes folder, we going to do git clone.\n1 git clone https://github.com/ltdrdata/ComfyUI-Manager.git Start ComfyUI 1 2 cd /workspace/ComfyUI/ python main.py --listen Connect to ComfyUI Go back to Pod Console, click on the Connect button.\nNotice the Public IP and External port across from Internal port 8188 (ComfyUI default port).\nThat\u0026rsquo;s how you get you to comfyui in your browser:\nPublic IP:External\nPublic IP:40007\nDownload Models from Civic AI Make sure you close ComfyUI if it\u0026rsquo;s running.\nAt the Jupyter Terminal shortcut, Ctrl + c.\nGo to the ComfyUI models folder:\n1 cd /workspace/ComfyUI/models Checkpoint Models Go to checkpoint model directory:\n1 cd /workspace/ComfyUI/models/checkpoints You can list all that\u0026rsquo;s inside the folder with ls.\nGo to Civic AI and find a checkpoint model you like Make a Civic AI account (it\u0026rsquo;s free).\nFor the search option click on the Filters option.\nThese are all the options in the Filters option menu (make sure you scroll down for more options):\nMy chosen checkpoint, GhostMix, for this tutorial:\nClick on it to head to the main page of chosen checkpoint.\nright click on the Download button and select Copy Link.\nThe link address should be copied and it should be this:\n1 https://civitai.com/api/download/models/76907 Find out the file name of the check point by downloading it to local computer and cancel the download:\nFile name:\nghostmix_v20Bakedvae.safetensors\nDownload Chosen Checkpoint into your pod Head over to your pod Jupyter Notebook\u0026rsquo;s terminal.\nMake sure you are in the checkpoint folder.\n1 cd /workspace/ComfyUI/models/checkpoints Now download the chosen model:\n1 wget -O ghostmix_v20Bakedvae.safetensors https://civitai.com/api/download/models/76907 So the format is:\n1 wget -O filename_with_extension model_url Type ls to make sure the model is downloaded.\nStarting Workflow Start up Python\u0026rsquo;s virtual environment 1 source venv/bin/activate Start up ComfyUI 1 2 cd /workspace/ComfyUI/ python main.py --listen `\nConclusion So that\u0026rsquo;s basically it for the set up.\nNow you need a workflow and download models that the workflow require. I\u0026rsquo;ll make another post on AnimateDiff workflow using this base setup.\nCredits Vampire Elf picture: created by me using Stable Diffusion and A1111. ComfyUI AnimateDiff Prompt Travel: Runpod.io Cloud GPUs Tutorial by c0nsumption ","date":"2024-01-21T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/stable-diffusion-in-the-cloud-set-up/vampire_elf_hu698cc1e37a206339e30a863b9b5838e5_69657_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/stable-diffusion-in-the-cloud-set-up/","title":"Stable Diffusion in the Cloud Set up"},{"content":"Current Webstack 2023 Elixir I enjoy the programming language. It\u0026rsquo;s fun and it\u0026rsquo;s mostly complete as quoted by Jose Valim (the creator of Elixir).\nThere are mostly improvements for developers like debugging and bug fixes.\nJose Valim have been working hard and in a few years or so maybe we get type in Elixir. It\u0026rsquo;s not guarantee but he have been talking about his work.\nOne thing is there are like three version managers for managing Elixir and Erlang versions now.\nasdf - install and manage different Elixir and Erlang versions kiex - install and manage different Elixir versions kerl - install and manage different Erlang versions Phoenix Framework It\u0026rsquo;s getting better.\nDeployment is mix release 1 instead of using edeliver.\nThe framework have built in log-in now. Back then there were so many third party log-in options. It was hectic just trying to figure it out and integrate it.\nJavascript Just plain vanilla Javascript. I am not really into front end as much and don\u0026rsquo;t have much time for it. Been doing a lot of statistic, data science, and machine learning stuff.\nJQuery I\u0026rsquo;m going to stick with what I know. End users doesn\u0026rsquo;t care about the tech you use. It works and I\u0026rsquo;d like to quickly bring up a project.\nBootstrap Same reason as above.\nPostgreSQL PostgreSQL is still my favorite RMDB.\nLinux (Debian) Debian is rock solid as a server OS. It\u0026rsquo;s either this or Ubuntu. It doesn\u0026rsquo;t matter much for me.\nIDE VIM \u0026amp; Tmux These two in tandem is my go to for web development.\nFuture Tailwind CSS I\u0026rsquo;d like to move to Tailwind CSS. They already name all the things you need and you don\u0026rsquo;t have to come up with any. It may looks ugly html but I don\u0026rsquo;t mind it. I don\u0026rsquo;t mind html tag having tons of classes.\nIt also remove zombie classes too. Zombie classes are classes that are defined but not used.\nPhoenix LiveView Can reduce Javascript codes. I\u0026rsquo;d like to use as little javascript as possible and more Elixir instead.\nIt\u0026rsquo;s not 1.0 yet and even then it\u0026rsquo;s probably not perfect. I\u0026rsquo;ll play around just to stay ahead and keep a general idea where it\u0026rsquo;s going.\nIDE VS Studio Code It\u0026rsquo;s a good IDE for tailwind css integration (autocomplete and suggestions).\nFly.io Manage server and back end for me. Less system admin tasks eating my time and it\u0026rsquo;ll let me concentrate on iterating and getting the project out.\nSummary At the end of the day, I am tired of trying to get the ideal tech stack and get everything perfect.\nI just want to have a product out.\nGet people to use it and iterate the product.\nOnce and if the product is flesh out then I\u0026rsquo;ll worry about perfection.\nAt this point my mindset is my code will be toss out within 5 years for a rewrite either adopting a new tech or update. So don\u0026rsquo;t worry about perfection.\nCredit Ghost In The Shell Ghost Shell ","date":"2023-11-17T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/my-web-stack-in-the-year-2023-and-the-future/ghost-in-the-shell-7765132_1280_hu2ae759ce12b03dce750ac36f7d6b378c_337571_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/my-web-stack-in-the-year-2023-and-the-future/","title":"My Web Stack in the year 2023 and the future"},{"content":"Intro I\u0026rsquo;ve been doing side project. One of the area I am interested in financial securities mostly stocks, ETFs, REITs, and options.\nI came across congress financial security trading records. I cleaned the data and currently figuring out what the response / outcome column is.\nI came across a few R packages along the way that have been useful for me for this endearvour and I\u0026rsquo;d like to share them. All these R packages was found through a youtube tutorial.\nPackages RQuantLib Package This package is very useful for getting and finding all trading dates of the US market.\nExample:\n1 2 3 4 5 6 # 2023-11-15 - Wednesday # 2023-11-12 - Sunday RQuantLib::isBusinessDay( calendar = \u0026#34;UnitedStates/NYSE\u0026#34;, dates = as.Date(c(\u0026#39;2023-11-15\u0026#39;,\u0026#39;2023-11-12\u0026#39;))) # Output: [1] TRUE FALSE Source: https://github.com/eddelbuettel/rquantlib\nOf course Dirk Eddelbuettel is involved. The dude is a legend in R circle (and anything related to C++ and R crossover).\nrvest Package It\u0026rsquo;s a webscraping package.\nI used this to grab the EPS and earning dates to create a response/outcome variable for congress trading dataset.\nCredit for the code. quantRoom youtube.\nExample:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ### # get EPS by Ticker - includes Earning date ### getEPS \u0026lt;- function(ticker) { Sys.sleep(3) url \u0026lt;- paste0( \u0026#34;https://finance.yahoo.com/calendar/earnings/?symbol=\u0026#34;, ticker) # read in page html pg \u0026lt;- rvest::read_html(url) # locate table TABLE \u0026lt;- pg %\u0026gt;% rvest::html_nodes(\u0026#34;table\u0026#34;) %\u0026gt;% rvest::html_table() # row bind results and convert to data.frame TABLE \u0026lt;- as.data.frame(t(do.call(rbind,TABLE[[1]]))) # remove timezone from Earnings Date TABLE$`Earnings Date` \u0026lt;- gsub(\u0026#34;EST\u0026#34;,\u0026#34;\u0026#34;,TABLE$`Earnings Date`) TABLE$`Earnings Date` \u0026lt;- gsub(\u0026#34;EDT\u0026#34;,\u0026#34;\u0026#34;,TABLE$`Earnings Date`) # fix Earning Date/time TABLE$`Earnings Date` \u0026lt;- as.POSIXct(TABLE$`Earnings Date`, format = \u0026#34;%b %d, %Y, %I %p\u0026#34;, tz = \u0026#34;EST\u0026#34;) # fix EPS TABLE$`EPS Estimate` \u0026lt;- as.numeric(TABLE$`EPS Estimate`) %\u0026gt;% suppressWarnings() TABLE$`Reported EPS` \u0026lt;- as.numeric(TABLE$`Reported EPS`) %\u0026gt;% suppressWarnings() # fix earning surprise percentage TABLE$`Surprise(%)` \u0026lt;- ( as.numeric(gsub(\u0026#34;\\\\+\u0026#34;,\u0026#34;\u0026#34;,TABLE$`Surprise(%)`)) %\u0026gt;% suppressWarnings() )/100 # return ALL TABLE } EPS \u0026lt;- getEPS(ticker=\u0026#34;ADM\u0026#34;) Source: https://rvest.tidyverse.org/\nCredits Pictures Bear \u0026amp; Baby https://pixabay.com/illustrations/toddler-baby-teddy-bear-cute-stars-8297939/ Gifts https://pixabay.com/photos/gift-package-ribbon-parcel-444520/ Code tutorial quantRoom youtube. ","date":"2023-11-15T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/useful-r-packages-i-ve-come-across/gift-444520_640_hu8aa387175768c8b2bcaf144681e43f89_89602_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/useful-r-packages-i-ve-come-across/","title":"Useful Financial R packages I've come across"},{"content":"Step 1: Create your Digital Ocean droplet Make sure you add your SSH when creating digital ocean and do not add a paraphrase to your SSH. It doesn’t work with paraphrase.\nStep 2: Setting up your server (DO droplet) Log into your server. 1 ssh root@134.209.8.141 Setup VIM as default editor on your server. 1 sudo update-alternatives --config editor Make sure you select the /usr/bin/vim.basic option which is number 3.\nUpdate your locale. 1 2 sudo update-locale LC_ALL=en_US.UTF-8 sudo update-locale LANGUAGE=en_US.UTF-8 Create new deploy user The next step is to create a non-root user on the server which will own our application and handle deployments. We’ll configure the .ssh directory for the user, configure the user to have passwordless sudo access, and disabled password login to harden the server a bit.\nLet’s create the user deploy.\nAs root on the target server:\n1 adduser deploy Fill out the data as required. Like password.\nNext, run this series of commands to configure the user’s .ssh directory:\n1 2 3 4 5 sudo mkdir -p /home/deploy/.ssh sudo touch /home/deploy/.ssh/authorized_keys sudo chmod 700 /home/deploy/.ssh sudo chmod 644 /home/deploy/.ssh/authorized_keys sudo chown -R deploy:deploy /home/deploy As root on the target server:\n1 sudo vim /etc/ssh/sshd_config Verify that password authentication is set to “no”:\n1 PasswordAuthentication no Add deploy to sudoers Finally we\u0026rsquo;ll add the user to sudo.\nAs root on the target machine:\n1 visudo This will open the sudoers file where we can add the deploy users privileges below root like this:\n1 2 3 4 5 # User privilege specification root ALL=(ALL:ALL) ALL deploy ALL=(ALL) NOPASSWD: ALL Add ssh public key to authorized_keys In another terminal window, on your local machine:\n1 cat ~/.ssh/id_rsa.pub On the target server, paste the key into authorized_keys:\n1 sudo vim /home/deploy/.ssh/authorized_keys Now we can safely log in and work as the deploy user on the target machine without having to enter a password.\nNow go to Basic Hardening article to remove root login.\nSet up your firewall on Ubuntu source/credit: https://www.digitalocean.com/community/tutorials/initial-server-setup-with-ubuntu-18-04\nUbuntu 18.04 servers can use the UFW firewall to make sure only connections to certain services are allowed. We can set up a basic firewall very easily using this application.\nDifferent applications can register their profiles with UFW upon installation. These profiles allow UFW to manage these applications by name. OpenSSH, the service allowing us to connect to our server now, has a profile registered with UFW.\nYou can see this by typing:\n1 ufw app list Output:\n1 2 Available applications: OpenSSH We need to make sure that the firewall allows SSH connections so that we can log back in next time. We can allow these connections by typing:\n1 ufw allow OpenSSH Afterwards, we can enable the firewall by typing:\n1 ufw enable Type “y” and press ENTER to proceed. You can see that SSH connections are still allowed by typing:\n1 ufw status Output:\n1 2 3 4 5 6 Status: active To Action From -- ------ ---- OpenSSH ALLOW Anywhere OpenSSH (v6) ALLOW Anywhere (v6) As the firewall is currently blocking all connections except for SSH, if you install and configure additional services, you will need to adjust the firewall settings to allow acceptable traffic in. You can learn some common UFW operations in this guide.\nStep 3: Target Server - Install asdf Sometimes, you’ll need to install specific versions or Erlang, Elixir or Node, and we’ll use asdf, a version manager to tackle this complex task.\nasdf is an extendable version manager with support for Ruby, Node.js, Elixir, Erlang \u0026amp; more.\nTo install it, we’ll first switch over to our newly created deploy user.\nOn the target server as root:\n1 2 $ su deploy $ cd # move into deploy\u0026#39;s home path Install asdf Install dependencies:\n1 2 3 4 5 6 7 8 9 10 11 12 13 sudo add-apt-repository \u0026#34;deb http://archive.ubuntu.com/ubuntu $(lsb_release -sc) main universe restricted multiverse\u0026#34; sudo add-apt-repository \u0026#34;deb http://archive.ubuntu.com/ubuntu $(lsb_release -sc) universe\u0026#34; sudo apt-get update sudo apt-get install -y build-essential git wget libssl-dev libreadline-dev \\ libncurses5-dev zlib1g-dev m4 curl wx-common libwxgtk3.0-dev autoconf \\ libxml2-utils xsltproc fop unixodbc unixodbc-bin unixodbc-dev sudo apt-get install openjdk-8-jdk sudo apt-get autoremove sudo apt-get upgrade Ensure that you are using the deploy account, and clone the repo:\n1 git clone https://github.com/asdf-vm/asdf.git ~/.asdf After this you need to add asdf to your deploy user bash profile:\n1 2 cd ~ vim ~/.profile Add this at the end of your profile:\n1 . $HOME/.asdf/asdf.sh Optional step if you are on $5 Digital Ocean Droplet For $5 droplet, asdf only works if you create a swapspace (I set it to 4GB) and wait a bit for asdf to install and build erlang felt like 30 min. See solution: (https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-18-04) See the problem reported: (https://github.com/asdf-vm/asdf-erlang/issues/91)\nSwitch to root to create swap space.\nWe can see if the system has any configured swap by typing:\n1 sudo swapon --show You can verify that there is no active swap using the free utility:\n1 free -h Create swap file. The size of swap file is usually double your ram. But do 4G minimum.\n1 sudo fallocate -l 4G /swapfile We can verify that the correct amount of space was reserved by typing:\n1 ls -lh /swapfile Enabling the swap file Now that we have a file of the correct size available, we need to actually turn this into swap space.\nFirst, we need to lock down the permissions of the file so that only the users with root privileges can read the contents. This prevents normal users from being able to access the file, which would have significant security implications.\nMake the file only accessible to root by typing:\n1 sudo chmod 600 /swapfile Verify the permissions change by typing:\n1 ls -lh /swapfile As you can see, only the root user has the read and write flags enabled.\nWe can now mark the file as swap space by typing:\n1 sudo mkswap /swapfile Output:\n1 2 Setting up swapspace version 1, size = 1024 MiB (1073737728 bytes) no label, UUID=6e965805-2ab9-450f-aed6-577e74089dbf After marking the file, we can enable the swap file, allowing our system to start utilizing it:\n1 sudo swapon /swapfile Verify that the swap is available by typing:\n1 sudo swapon --show Making the swapfile permanent Our recent changes have enabled the swap file for the current session. However, if we reboot, the server will not retain the swap settings automatically. We can change this by adding the swap file to our /etc/fstab file.\nBack up the /etc/fstab file in case anything goes wrong:\n1 sudo cp /etc/fstab /etc/fstab.bak Add the swap file information to the end of your /etc/fstab file by typing:\n1 echo \u0026#39;/swapfile none swap sw 0 0\u0026#39; | sudo tee -a /etc/fstab Now switch to deploy user and check to see if asdf by typing asdf.\nStep 4: Target Server - Install Erlang/Elixir Because we need our Phoenix project to run on both the local development machine and the production server, we’ll need to install the same languages and tools in both places. Erlang 21.1, and Elixir 1.7.4.\nInstall Erlang We’re going to use asdf to install Erlang. I uses plugins for different libraries, so let’s add the plugin:\nAs deploy on the target machine:\n1 asdf plugin-add erlang Install Erlang/OTP 22 (or whichever version your app needs)\n1 2 asdf install erlang 22.0 asdf global erlang 22.0 OPTIONAL If anything went wrong or if dependencies skipped\n1 2 3 4 asdf plugin-remove erlang asdf plugin-remove elixir asdf plugin-add erlang asdf plugin-add elixir Install Elixir As deploy add the plugin:\n1 asdf plugin-add elixir Install Elixir and make it global:\n1 2 asdf install elixir 1.8.2 asdf global elixir 1.8.2 Now you can open a new terminal and try erl:\n1 erl And you can try iex:\n1 iex Use asdf .tool-versions file to manage which version is active on each of your projects.\nUse Mix to install Hex.\n1 mix local.hex Step 5: Target Server - Install Nodejs Installing Node is straightforward with a few commands.\nNOTE: Look up what NODE version to install. Replace XX with version number such as 12. see https://github.com/nodesource/distributions/blob/master/README.md\n1 2 3 sudo apt install curl curl -sL https://deb.nodesource.com/setup_XX.x | sudo -E bash - sudo apt-get install -y nodejs Step 6: Target Server - Install Postgresql base on https://computingforgeeks.com/install-postgresql-11-on-ubuntu-18-04-ubuntu-16-04/\nAs root on the target server:\nAdd PostgreSQL 11 APT repository (for Bionic 18.04 ubuntu)\nhttps://www.postgresql.org/download/linux/ubuntu/\nCreate the file /etc/apt/sources.list.d/pgdg list and add a line for the repository.\n1 sudo vim /etc/apt/sources.list.d/pgdg.list Add this line:\n1 deb http://apt.postgresql.org/pub/repos/apt/ bionic-pgdg main Import the repository signing key, and update the package lists :\n1 2 wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - sudo apt-get update Install PostgreSQL 11 on Ubuntu 18.04 / Ubuntu 16.04 After importing GPG key, add repository contents to your Ubuntu 18.04/16.04 system:\n1 sudo apt-get install postgresql-11 Verify repository file contents\n1 cat /etc/apt/sources.list.d/pgdg.list Create new database user During the postgres installation, a postgres root user postgres was created, but we don’t want to connect to the server with this user. Let’s create a separate postgres user for our app.\nYou can switch to the postgres user with su postgres than back to root with exit\nOn the target server, switch to the postgres user, create a phx database user and set the new user’s password:\n1 2 3 4 5 6 $ su postgres $ cd ~ $ createuser phx --pwprompt Enter password for new role: Enter it again: NOTE: Remember these credentials as they’ll be used once we configure our Phoenix application for production.\nCreate production database: We’ll need to create our production database manually as edeliver only manages migrations.\nAs postgres on the target server:\n1 createdb fumigate_prod Then log in to psql:\n1 psql Ensure that you are logged into the postgres cli tool and run:\n1 GRANT ALL PRIVILEGES ON DATABASE fumigate_prod TO phx; This gives our new phx user access to the newly created database. Exit psql with \\q.\nDatabase credentials: To re-cap:\nusername: phx password: \u0026lt;yourpass\u0026gt; database: fumigate_prod Step 7: Target Server - Production Configuration Create prod.secret.exs You may have noticed that Phoenix created a config/prod.secret.exs file. This is imported by config/prod.exs but is ignored by git by default. We’ll need to create this file on the target machine so edeliver can symlink to it during the build process.\nWe’re also going to store our applications in ~/apps/\u0026lt;appname\u0026gt; format which is the equivalent of /home/deploy/apps/\u0026lt;appname\u0026gt;.\nLet’s switch over to our deploy user. On the target machine:\n1 2 su deploy cd Make a new directory for our secrets:\n1 mkdir -p apps/fumigate/secret Then create a new prod.secret.exs file in that directory:\n1 vim ~/apps/fumigate/secret/prod.secret.exs Add this content to it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 use Mix.Config config :fumigate, FumigateWeb.Endpoint, secret_key_base: \u0026#34;YOURKEYHERE+v7fAdFNvoSZpUwTU96jA0UjEF1sgiVwdI5F\u0026#34; config :fumigate, Fumigate.Repo, username: \u0026#34;phx\u0026#34;, password: \u0026#34;yourpassword\u0026#34;, database: \u0026#34;fumigate_prod\u0026#34;, pool_size: 15 You can generate a new secret key by using mix phx.gen.secret on your local machine.\nIMPORTANT: Update the file to have a production ready secret key, and the database credentials you set from earlier.\nStep 8: Install Distillery and edeliver As previously mentioned, Distillery compiles our Phoenix application into releases, and edeliver uses ssh and scp to build and deploy the releases to our production server.\nOn your local machine, open mix.exs and add 2 deps:\n1 2 {:edeliver, \u0026#34;\u0026gt;= 1.6.0\u0026#34;}, {:distillery, \u0026#34;~\u0026gt; 2.0\u0026#34;, warn_missing: false}, And add :edeliver to extra_applications in the application block:\n1 2 3 4 5 6 def application do [ mod: {Fumigate.Application, []}, extra_applications: [:logger, :runtime_tools, :edeliver] ] end Use mix to install deps:\n1 mix deps.get Initialize Distillery Distillery requires a build configuration file that is not generated by default.\nLet’s generate it with:\n1 mix distillery.init This generates configuration files for Distillery in the rel directory. We don’t need to make any changes to the default config.\nWe now need to exclude .deliver/releases/ from our git repo.\n1 echo \u0026#34;.deliver/releases/\u0026#34; \u0026gt;\u0026gt; .gitignore Configure edeliver Create a .deliver directory in your project folder and add the config file: (Change the IP to your server IP 134.209.8.141)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 #!/bin/bash APP=\u0026#34;fumigate\u0026#34; BUILD_HOST=\u0026#34;134.209.8.141\u0026#34; BUILD_USER=\u0026#34;deploy\u0026#34; BUILD_AT=\u0026#34;/tmp/edeliver/$APP/builds\u0026#34; START_DEPLOY=true CLEAN_DEPLOY=true # prevent re-installing node modules; this defaults to \u0026#34;.\u0026#34; GIT_CLEAN_PATHS=\u0026#34;_build rel priv/static\u0026#34; PRODUCTION_HOSTS=\u0026#34;134.209.8.141\u0026#34; PRODUCTION_USER=\u0026#34;deploy\u0026#34; DELIVER_TO=\u0026#34;/home/deploy/apps\u0026#34; # For Phoenix projects, symlink prod.secret.exs to our tmp source pre_erlang_get_and_update_deps() { local _prod_secret_path=\u0026#34;/home/deploy/apps/$APP/secret/prod.secret.exs\u0026#34; if [ \u0026#34;$TARGET_MIX_ENV\u0026#34; = \u0026#34;prod\u0026#34; ]; then status \u0026#34;Linking \u0026#39;$_prod_secret_path\u0026#39;\u0026#34; __sync_remote \u0026#34; [ -f ~/.profile ] \u0026amp;\u0026amp; source ~/.profile mkdir -p \u0026#39;$BUILD_AT\u0026#39; ln -sfn \u0026#39;$_prod_secret_path\u0026#39; \u0026#39;$BUILD_AT/config/prod.secret.exs\u0026#39; \u0026#34; fi } pre_erlang_clean_compile() { status \u0026#34;Running npm install\u0026#34; __sync_remote \u0026#34; [ -f ~/.profile ] \u0026amp;\u0026amp; source ~/.profile set -e cd \u0026#39;$BUILD_AT\u0026#39;/assets npm install \u0026#34; status \u0026#34;Compiling assets\u0026#34; __sync_remote \u0026#34; [ -f ~/.profile ] \u0026amp;\u0026amp; source ~/.profile set -e cd \u0026#39;$BUILD_AT\u0026#39;/assets node_modules/.bin/webpack --mode production --silent \u0026#34; status \u0026#34;Running phoenix.digest\u0026#34; # log output prepended with \u0026#34;-----\u0026gt;\u0026#34; __sync_remote \u0026#34; # runs the commands on the build host [ -f ~/.profile ] \u0026amp;\u0026amp; source ~/.profile # load profile (optional) set -e # fail if any command fails (recommended) cd \u0026#39;$BUILD_AT\u0026#39; # enter the build directory on the build host (required) # prepare something mkdir -p priv/static # required by the phoenix.digest task # run your custom task APP=\u0026#39;$APP\u0026#39; MIX_ENV=\u0026#39;$TARGET_MIX_ENV\u0026#39; $MIX_CMD phx.digest $SILENCE APP=\u0026#39;$APP\u0026#39; MIX_ENV=\u0026#39;$TARGET_MIX_ENV\u0026#39; $MIX_CMD phx.digest.clean $SILENCE \u0026#34; } These are mostly environment variables use by Edeliver in the shell scripts. Go through them one by one and try to understand what they’re doing.\nYou can read more about the configuration variables in their Wiki.\nDon’t forget to update your host configuration\nThe custom functions are hooks that run during different phases of the deployment.\nYou can also read more about running additional tasks in their Wiki.\nProject prod configuration We’ll need to make some changes to the default production configuration in our project.\nDO NOT RUN ON PORT 80. KEEP IT AT 4000. The application is running on deploy there fore it cannot run on port 80. We need to route incoming requests from 80 http and https to port 4000. Do not attempt to run app as root. That’s asking for trouble. Source: https://elixirforum.com/t/running-on-ec2-giving-error/13574/5\nOpen config/prod.exs and update it to this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 use Mix.Config config :fumigate, FumigateWeb.Endpoint, http: [:inet6, port: System.get_env(\u0026#34;PORT\u0026#34;) || 4000], url: [host: \u0026#34;fumigatedb.com\u0026#34;, port: 80], cache_static_manifest: \u0026#34;priv/static/cache_manifest.json\u0026#34;, server: true, code_reloader: false # Do not print debug messages in production config :logger, level: :info # ## SSL Support # # To get SSL working, you will need to add the `https` key # to the previous section and set your `:url` port to 443: # # config :fumigate, FumigateWeb.Endpoint, # ... # url: [host: \u0026#34;example.com\u0026#34;, port: 443], # https: [ # :inet6, # port: 443, # cipher_suite: :strong, # keyfile: System.get_env(\u0026#34;SOME_APP_SSL_KEY_PATH\u0026#34;), # certfile: System.get_env(\u0026#34;SOME_APP_SSL_CERT_PATH\u0026#34;) # ] # # The `cipher_suite` is set to `:strong` to support only the # latest and more secure SSL ciphers. This means old browsers # and clients may not be supported. You can set it to # `:compatible` for wider support. # # `:keyfile` and `:certfile` expect an absolute path to the key # and cert in disk or a relative path inside priv, for example # \u0026#34;priv/ssl/server.key\u0026#34;. For all supported SSL configuration # options, see https://hexdocs.pm/plug/Plug.SSL.html#configure/1 # # We also recommend setting `force_ssl` in your endpoint, ensuring # no data is ever sent via http, always redirecting to https: # # config :fumigate, FumigateWeb.Endpoint, # force_ssl: [hsts: true] # # Check `Plug.SSL` for all available options in `force_ssl`. # ## Using releases (distillery) # # If you are doing OTP releases, you need to instruct Phoenix # to start the server for all endpoints: # config :phoenix, :serve_endpoints, true # # Alternatively, you can configure exactly which server to # start per endpoint: # # config :fumigate, FumigateWeb.Endpoint, server: true # # Note you can\u0026#39;t rely on `System.get_env/1` when using releases. # See the releases documentation accordingly. # Finally import the config/prod.secret.exs which should be versioned # separately. import_config \u0026#34;prod.secret.exs\u0026#34; These are settings recommended by Distillery. You might notice that we’re setting the system port using the PORT environment variable. This needs to be available during the build process and we can add it to the ~/.profile file on our production server.\nAdd env Variables to Target Server Login as deploy to the target server.\nOpen up the .profile.\n1 2 cd ~ vim ~/.profile And add these lines:\n1 2 export MIX_ENV=prod export PORT=4000 This will ensure that our system runs on port 4000, and that the environment is set to prod.\nLogout of deploy and log back in for ~/.profile be in effect.\nStep 9: Deployment We’re finally ready to build and deploy our first release. The default version is 0.1.0 so let’s just keep that for now.\nWarning: the commands for building and deploy can seem complicated at first but once you become familiar with them, you will see that they use a natural language style.\nBuild the production release: NOTE: edeliver uses git master branch to build the code into release package.\nIn your project directory and git master branch:\n1 mix edeliver build release production --verbose This builds the release on the target server, and stores the archive in your local .edeliver/releases directory.\nDeploy the reload to production: In your project directory:\n1 2 3 mix edeliver deploy release to production --verbose mix edeliver migrate production --verbose mix edeliver start production --verbose Other neat edeliver commands:\n1 2 3 4 5 mix edeliver ping production # shows which nodes are up and running mix edeliver version production # shows the release version running on the nodes mix edeliver show migrations on production # shows pending database migrations mix edeliver migrate production # run database migrations mix edeliver restart production # or start or stop You can read more about Edeliver in their documentation.\nView our website on our server 1 2 ssh deploy@188.166.182.170 curl localhost:4000 Step 10: Generating SSL CERT - BEFORE Routing incoming http request to port 4000 (our phoenix app) https://www.digitalocean.com/community/tutorials/how-to-use-certbot-standalone-mode-to-retrieve-let-s-encrypt-ssl-certificates-on-ubuntu-1804\nhttps://github.com/certbot/certbot/issues/5257\nWe should generate SSL Certificate. The certbot generator needs port 80. So you need to do this before rerouting incoming port 80 requests to our phoenix web app that’s listening on port 4000.\nStep 1 — Installing Certbot The first step to using Let’s Encrypt to obtain an SSL certificate is to install the Certbot software on your server.\nCertbot is in very active development, so the Certbot packages provided by Ubuntu tend to be outdated. However, the Certbot developers maintain a Ubuntu software repository with up-to-date versions, so we’ll use that repository instead.\nFirst, add the repository:\n1 2 sudo add-apt-repository ppa:certbot/certbot apt-get update Install certbot:\n1 sudo apt-get install certbot Step 2 — Generating Certificate Make sure that your firewall is accepting port 80.\n1 sudo ufw allow 80 Generate the SSL cert:\n1 sudo certbot certonly --standalone --preferred-challenges http -d fumigatedb.com -d www.fumigatedb.com Enable it so deploy user and app own by deploy can read those certs:\n1 2 chmod 755 /etc/letsencrypt/live/ chmod 755 /etc/letsencrypt/archive/ We’ll get back to using cert on the server soon in step 13.\nStep 11: Routing incoming http request to port 4000 (our phoenix app) https://serverfault.com/questions/238563/can-i-use-ufw-to-setup-a-port-forward\nLet’s say you want to forward requests going to 80 (http) to a server listening on port 4000.\nNote that you will need to make sure port 8080 is allowed, otherwise ufw will block the requests that are redirected to 4000.\n1 sudo ufw allow 4000/tcp There are no ufw commands for setting up the port forwards, so it must be done via configuraton files. Add the lines below to /etc/ufw/before.rules, before the filter section, right at the top of the file:\n1 sudo vim /etc/ufw/before.rules 1 2 3 4 *nat :PREROUTING ACCEPT [0:0] -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 4000 COMMIT Then restart the server:\n1 2 sudo reboot ssh deploy@134.209.8.141 Enable ufw to start on boot:\n1 sudo ufw enable Debug tips: check /var/log/syslog\nStep 12: Fail2ban https://www.lifewire.com/harden-ubuntu-server-security-4178243\nhttps://www.techrepublic.com/article/how-to-install-fail2ban-on-ubuntu-server-18-04/\nThe fail2ban system is an intrusion prevention system that monitors log files and searches for particular patterns that correspond to a failed login attempt. If a certain number of failed logins are detected from a specific IP address (within a specified amount of time), fail2ban will block access from that IP address.\nTo install fail2ban, open a terminal window and issue the command:\n1 2 3 sudo apt-get update sudo apt-get upgrade sudo apt-get install -y fail2ban Within the directory /etc/fail2ban, you’ll find the main configuration file, jail.conf. Also in that directory is the subdirectory, jail.d. The jail.conf file is the main configuration file and jail.d contains the secondary configuration files. Do not edit the jail.conf file. Instead, we’ll create a new configuration that will monitor SSH logins with the command:\n1 sudo vim /etc/fail2ban/jail.local In this new file add the following contents:\n1 2 3 4 5 6 [sshd] enabled = true port = 22 filter = sshd logpath = /var/log/auth.log maxretry = 3 This configuration does the following:\nEnables the jail. Sets the SSH port to be monitored to 22. Uses the sshd filter. Sets the log file to be monitored. Save and close that file. Restart fail2ban with the command:\n1 2 3 sudo systemctl start fail2ban sudo systemctl enable fail2ban sudo systemctl restart fail2ban If you attempt to Secure Shell into that server and fail the log in three times (set as the default by fail2ban), access will be then blocked from the IP address you are working from.\nTesting and unbanning You can test to make sure the new jail works by failing three attempts at logging into the server, via ssh. After the third failed attempt, the connection will hang. Hit [Ctrl]+[c] to escape and then attempt to SSH back into the server. You should no longer be able to SSH into that server from the IP address you were using.\nYou can then unban your test IP address with the following command:\n1 sudo fail2ban-client set sshd unbanip IP_ADDRESS where IP_ADDRESS is the banned IP Address.\nYou should now be able to log back into the server with SSH.\nScratching the surface This barely scratches the surface as to what fail2ban can do. But now you have a good idea on how to use the system. To find out more, make sure to read the man page with the command:\n1 man fail2ban That manual page provides a good overview of what fail2ban can do.\nStep 13: Port foward SSL to Port 4001 Set up the firewall:\n1 2 3 sudo ufw allow https sudo ufw allow 443 sudo ufw allow 4001 There are no ufw commands for setting up the port forwards, so it must be done via configuraton files. Add the lines below to /etc/ufw/before.rules, before the filter section, right at the top of the file:\n1 sudo vim /etc/ufw/before.rules 1 2 3 4 *nat :PREROUTING ACCEPT [0:0] -A PREROUTING -p tcp --dport 443 -j REDIRECT --to-port 4001 COMMIT Then restart the server:\n1 2 3 sudo ufw enable sudo reboot ssh deploy@134.209.8.141 Step 14: SSL https://blog.progressplum.app/ssl-migration-from-nginx-to-cowboy-2-in-phoenix-1-4/\nhttps://medium.com/@zek/secure-your-phoenix-app-with-free-ssl-48ac749c17d7\nStep 1 — Generating Diffie Hellman parameters If you don’t already have a set of Diffie Hellman parameters[2] to use with your SSL, generate a new set for extra security. Run this command on the server but be aware that it’s very CPU-intensive and may take a while on a slow VPS.\n1 openssl dhparam -out /etc/letsencrypt/dhparam.pem 4096 Step 2 — Set up your environment 1 sudo vim ~/.profile 1 2 3 4 5 6 7 8 9 10 11 12 13 export MIX_ENV=prod export PORT=4000 export SPORT=4001 export SSL_CERT_FILE=/etc/letsencrypt/live/fumigatedb.com/cert.pem export SSL_CACERT_FILE=/etc/letsencrypt/live/fumigatedb.com/chain.pem export SSL_KEY_FILE=/etc/letsencrypt/live/fumigatedb.com/privkey.pem export SSL_DHPARAM_FILE=/etc/letsencrypt/dhparam.pem Change fumigatedb.com to your domainname.\nStep 3 — prod Configuration Update your application’s Endpoint configuration to add SSL support in config/prod.exs.\n1 vim config/prod.exs 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 config :fumigate, FumigateWeb.Endpoint, url: [scheme: \u0026#34;https\u0026#34;, host: \u0026#34;fumigatedb.com\u0026#34;, port: 443], http: [:inet6, port: System.get_env(\u0026#34;PORT\u0026#34;) || 4000], https: [ :inet6, port: System.get\\_env(\u0026#34;SPORT\u0026#34;) || 4001, otp_app: :fumigate, cipher_suite: :strong, keyfile: System.get_env(\u0026#34;SSL_KEY_FILE\u0026#34;), certfile: System.get_env(\u0026#34;SSL_CERT_FILE\u0026#34;), cacertfile: System.get_env(\u0026#34;SSL_CACERT_FILE\u0026#34;), dhfile: System.get_env(\u0026#34;SSL_DHPARAM_FILE\u0026#34;) ], cache_static_manifest: \u0026#34;priv/static/cache_manifest.json\u0026#34;, server: true, code_reloader: false Save to git master repo and rerun edeliver again.\n1 2 3 4 5 6 git commit -a -m \u0026#34;edited prod.exs to add HTTPS scheme\u0026#34; git push origin master mix edeliver build release production --verbose mix edeliver deploy release to production mix edeliver stop production mix edeliver start production Check the ssl website now with: https://www.ssllabs.com/ssltest/\nStep 4 — Application Configuration forcing SSL 1 2 3 4 5 6 7 8 9 10 11 git commit -a -m \u0026#34;edited prod.exs to force SSL connection\u0026#34; git push origin master mix edeliver build release production --verbose mix edeliver deploy release to production mix edeliver stop production mix edeliver start production Step 5 — Increasing the security of SSL https://elixirforum.com/t/making-ssl-tests-all-pass-for-phoenix-lets-encrypt/3507/11\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 use Mix.Config # For production, don\u0026#39;t forget to configure the url host # to something meaningful, Phoenix uses this information # when generating URLs. # # Note we also include the path to a cache manifest # containing the digested version of static files. This # manifest is generated by the `mix phx.digest` task, # which you should run after static files are built and # before starting your production server. config :fumigate, FumigateWeb.Endpoint, url: [scheme: \u0026#34;https\u0026#34;, host: \u0026#34;fumigatedb.com\u0026#34;, port: 443], force_ssl: [hsts: true], http: [:inet6, port: System.get_env(\u0026#34;PORT\u0026#34;) || 4000], https: [ :inet6, port: System.get_env(\u0026#34;SPORT\u0026#34;) || 4001, otp_app: :fumigate, cipher_suite: :strong, keyfile: System.get_env(\u0026#34;SSL_KEY_FILE\u0026#34;), certfile: System.get_env(\u0026#34;SSL_CERT_FILE\u0026#34;), cacertfile: System.get_env(\u0026#34;SSL_CACERT_FILE\u0026#34;), dhfile: System.get_env(\u0026#34;SSL_DHPARAM_FILE\u0026#34;), versions: [:\u0026#34;tlsv1.2\u0026#34;, :\u0026#34;tlsv1.1\u0026#34;, :\u0026#34;tlsv1\u0026#34;], ciphers: ~w( ECDHE-ECDSA-AES256-GCM-SHA384 ECDHE-ECDSA-AES256-SHA384 ECDHE-ECDSA-AES128-GCM-SHA256 ECDHE-ECDSA-AES128-SHA256 ECDHE-ECDSA-AES256-SHA ECDHE-ECDSA-AES128-SHA ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES256-SHA ECDHE-RSA-AES128-SHA ECDH-ECDSA-AES256-GCM-SHA384 ECDH-ECDSA-AES256-SHA384 ECDH-ECDSA-AES128-GCM-SHA256 ECDH-ECDSA-AES128-SHA256 DHE-RSA-AES256-GCM-SHA384 DHE-RSA-AES256-SHA256 DHE-DSS-AES256-GCM-SHA384 DHE-DSS-AES256-SHA256 DHE-RSA-AES256-SHA DHE-DSS-AES256-SHA DHE-DSS-AES128-GCM-SHA256 DHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES128-SHA256 DHE-DSS-AES128-SHA256 DHE-RSA-AES128-SHA DHE-DSS-AES128-SHA AES128-GCM-SHA256 AES128-SHA DES-CBC3-SHA )c, secure_renegotiate: true, reuse_sessions: true, honor_cipher_order: true, client_renegotiation: false, eccs: [ :sect571r1, :sect571k1, :secp521r1, :brainpoolP512r1, :sect409k1, :sect409r1, :brainpoolP384r1, :secp384r1, :sect283k1, :sect283r1, :brainpoolP256r1, :secp256k1, :secp256r1, :sect239k1, :sect233k1, :sect233r1, :secp224k1, :secp224r1 ], ], cache_static_manifest: \u0026#34;priv/static/cache_manifest.json\u0026#34;, server: true, code_reloader: false # Do not print debug messages in production config :logger, level: :info # ## SSL Support # # To get SSL working, you will need to add the `https` key # to the previous section and set your `:url` port to 443: # # config :fumigate, FumigateWeb.Endpoint, # ... # url: [host: \u0026#34;example.com\u0026#34;, port: 443], # https: [ # :inet6, # port: 443, # cipher_suite: :strong, # keyfile: System.get_env(\u0026#34;SOME_APP_SSL_KEY_PATH\u0026#34;), # certfile: System.get_env(\u0026#34;SOME_APP_SSL_CERT_PATH\u0026#34;) # ] # # The `cipher_suite` is set to `:strong` to support only the # latest and more secure SSL ciphers. This means old browsers # and clients may not be supported. You can set it to # `:compatible` for wider support. # # `:keyfile` and `:certfile` expect an absolute path to the key # and cert in disk or a relative path inside priv, for example # \u0026#34;priv/ssl/server.key\u0026#34;. For all supported SSL configuration # options, see https://hexdocs.pm/plug/Plug.SSL.html#configure/1 # # We also recommend setting `force_ssl` in your endpoint, ensuring # no data is ever sent via http, always redirecting to https: # # # Check `Plug.SSL` for all available options in `force_ssl`. # ## Using releases (distillery) # # If you are doing OTP releases, you need to instruct Phoenix # to start the server for all endpoints: # config :phoenix, :serve_endpoints, true # # Alternatively, you can configure exactly which server to # start per endpoint: # # config :fumigate, FumigateWeb.Endpoint, server: true # # Note you can\u0026#39;t rely on `System.get_env/1` when using releases. # See the releases documentation accordingly. # Finally import the config/prod.secret.exs which should be versioned # separately. import_config \u0026#34;prod.secret.exs\u0026#34; Credit Base on these resources: https://devato.com/automate-elixir-phoenix-1-4-deployment-with-distillery-and-edeliver-on-ubuntu/#step-6-target-server-install-nodejs https://medium.com/@zek/deploy-early-and-often-deploying-phoenix-with-edeliver-and-distillery-part-one-5e91cac8d4bd https://medium.com/@zek/deploy-early-and-often-deploying-phoenix-with-edeliver-and-distillery-part-two-f361ef36aa10 https://blog.progressplum.app/ssl-migration-from-nginx-to-cowboy-2-in-phoenix-1-4/ https://elixirforum.com/t/elixir-1-9-releases-with-edeliver/23728/3?u=mythicalprogrammer Pictures First picture: Color Phoenix Tradition ","date":"2019-09-16T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/elixir-phoenix-deploy-distillery-and-edeliver/color-1544543_1280_hu03f7c0de7591ec620ffda9d2135553d8_169172_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/elixir-phoenix-deploy-distillery-and-edeliver/","title":"Elixir Phoenix 1.4 Deployments with Distillery and Edeliver on Ubuntu"},{"content":"Books I’ve read this week So I’m reading up my tech stack I’ve chosen for the next five years for my start up. Mostly setting up my dev machine and foundation toolings.\nAbsolute OpenBSD Unix for the Practical Paranoid (2nd Edition) by Michael W. Lucas\nThe partition harddrive during installing of this book is outdated.\nTutorial I’ve used is:\nhttps://sohcahtoa.org.uk/openbsd.html https://www.c0ffee.net/blog/openbsd-on-a-laptop/ I’ve installed xfce window manager desktop whatever and there were some hiccup. Especially having to figure out to enable an option for xfce terminal to run as shell login user (why is this not default?).\nI’ve also set up ksh terminal using polyglot github configuration. It works for me.\ntmux 2 Productive Mouse-Free Development by Brian P. Hogan\nI’ve read this book a few years ago and it’s the reason why I am good at tmux. I learned about tmux at a Ruby on Rail talk 10 years ago.\nI’m using this config file with a few tweaks. The pane split short cut for both horizontal and vertical are the same \u0026gt;___\u0026gt;. Also copy mode doesn’t use vi key even though pane navigation uses vi keys.\nI couldn’t get the font working on OpenBSD and am still working on it. It’s in the backburner.\nWhat am I reading next In the technology queue there are two books.\nI would like the latest Vim book. I’ve read the practical vim book and currently want to read up on neo vim and the latest book by the same author iirc. https://pragprog.com/book/modvim/modern-vim\nThe vim one will be handy since R studio is not a port in OpenBSD and the dependancies is pretty bad to make it port.\nI’ve found this sweeet article from my alma mata for vim+tmux R dev: https://hpcc.ucr.edu/manuals_linux-cluster_terminalIDE.html\nAlso forgot about this one: https://kadekillary.work/post/nvim-r/\nAlso a sweet website for top vim plugins: vim awesome\nAnother one I would like to read is a postgresql book. I’ll start with the book below and see how it goes.\nLearning PostgreSQL 10 - Second Edition by Andrey Volkov and Salahaldin Juba\nThere’s also a pretty interesting one in depth: Use the Index Luke\nFrom there I have an elixir book to read and a phoenix book but that’s for later.\nRandom thoughts on publishers It seems like I’m moving away from manning and have been reading a lot of Pragmatic Programmers and No Stach publisher.\nPackt is really a hit or miss.\nWhat about books in other subject? I’m reading a few self help books, which is personal, so I won’t be putting it here. Just trying to be my best self that’s all.\nAs for statistic, I’ve found a new spatial modeling book that was recently released: Geocomputation with R by Robin Lovelace, Jakub Nowosad, Jannes Muenchow\nI dislike the title name because it sounds funky to be honest I’m hoping it spatial analysis which from the summary it is. Maybe geocomputation is some domain specific terminology I don’t know and I’m just being silly.\nFeel free to hit me up on twitter, mastodon, and whatever if you guys have any suggestions on books either technical or self help or statistical modeling.\nHave a wonderful week.\nCredits First picture: https://pixabay.com/en/books-door-entrance-culture-1655783/ Second picture: https://pixabay.com/en/narrative-history-dream-tell-794978/ Third picture: https://pixabay.com/en/sunset-island-mar-dusk-brain-485016/ ","date":"2018-12-16T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/books-i-ve-read-this-week-and-future-reading-plans/books-1655783_1280_huf9341335e8f72240952417d2ba534169_608015_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/books-i-ve-read-this-week-and-future-reading-plans/","title":"Books I've read this week and future reading plans."},{"content":"Introduction This week I had to study for a job interview at a pretty sweet place. So in preparation for it I’ve read up on statistics that I’ve listed on my resume. I’ve came across a few good papers and I’m just happy to have read it. Just wanted to post it here in case if I ever need a refresher on Bayesian.\nBasic Master Level Statistic Introductory Mathematical Statistics Methods of Estimation and Properties of Point Estimators: Fundamental Exercises with Solutions by Dr. Olga Korosteleva\nThis is written by my professor and it was in a neat package of overview of what I’ve learned in my master program. It is a good refresher. There were some notes I’ve written for clarifications:\nMLE - This selects the parameter values that make the data most probable\rLikelihood (frequentist point of view) - is a function of the parameters of a statistical model, given specific observed data.\rMethod of Moments Estimators (MOM) \u0026 MLE are two estimator methods to do point estimation for parameter. This is the frequentist point of view on estimating parameters which are fixed. Where as Bayesian model parameter as a random variable and not a fixed point.\rFisher information - is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter theta of a distribution models X.\rCramer-Rao Lower Bound - expresses a lower-bound on the variance of unbiased estimators of a deterministic parameter. An unbiased estimator which achieves this lower bound is efficient.\rSufficient Statistic - a statistic that summarize all of the information in a sample about the desired parameter. (Penn State have more clarification on this).\rBayesian Review On Bayesian Data Analysis by Christian P. Robert and Judith Rousseau (August 27, 2018)\nPoint estimation parameter vs parameter with distribution. Credible Interval vs Confidence Interval. Critiques of Bayesian and solutions.\nAn Introduction to Bayesian Statistics Without Using Equations by Tomoharu Eguchi (2008)\nGreat visualization take on how to explain Bayesian Statistic.\nA tutorial on Bayesian nonparametric models by Samuel J. Gershman and David M. Blei (2012)\nA review what I used during my time at the FDA.\nCredits First picture: https://pixabay.com/en/book-magnifying-glass-glass-2304078/\n","date":"2018-12-13T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/interesting-papers-i-ve-read-this-week/book-2304078_1920_hudbbbaa23eb712c9e80c8b666d23f0fa6_671506_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/interesting-papers-i-ve-read-this-week/","title":"Interesting Papers I've read this week."},{"content":"Introduction I’ve been creating models for a while now.\nRecently read a statistical modeling book on survival analysis and I learned something new. Something that is thought provoking toward modeling.\nI just wanted to put my thoughts on here and see figure out where I’m going with this post when I get there.\nHypothesis, Response type, and Data type I recently have a growth or a better understanding between distinction of research and given data.\nIn the past I believe the data type is what play a role in the class and type of model you can do. Examples of type of model are longitudinal, survival, time series, etc…\nI know as a researcher you have to have a hypothesis first before conducting your experiment and choose your test statistic. If you don’t then you risk introducing bias.\nBut reading a recent book, I’ve come to realize that the hypothesis is the most important thing that dictate your model and the type of data you have at hand is a close second.\nWhat the type of your response in your model dictate your modeling class (longitudinal, survival, etc..). From there you have to make sure your data can address your hypothesis which is tie to the type of your response.\nAgain the hypothesis play a important role to modeling. It then dictate the response type which will dictate what type of data you should have. Then, the data type will dictate what model can you can do.\nBut real life isn’t like this.\nThe majority of data science and machine learning, you have data already. You cannot conduct experiment to generate data yourself because you are given data and expect to work on it. So you essentially skip several step and go straight to what type of data I have at hand and what class of model can I use. If you have a hypothesis it can help but if you have the wrong type of data you cannot answer it.\nBefore I give an example, I want to say most people actually get the data and throw it in a black box algorithm and see what’s going on. Another group of people will look at the data type and see what type of model you can apply. These two cases are mostly for when you are given data and the observations are done already.\nWhen you do research, you get to dictate what you’re looking via hypothesis. Then you get to design the experiment with the type of data you need in mind. Finally the classes of algorithm you can apply is base on your hypothesis and data type.\nBetween the given data and conducting research to get data, I am incline to believe the latter have less chances of bias.\nEtc\u0026hellip; First picture: https://pixabay.com/en/person-reading-studyin-bed-books-984236/\n","date":"2018-11-29T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/things-i-ve-learned-about-modeling/person-984236_1920_hu54856dd37b73ba3ac68393b7b83b222a_651196_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/things-i-ve-learned-about-modeling/","title":"Things I've learned about modeling."},{"content":"Introduction I recently talked to a buddy of mine and we had different view on technology. Being a veteran in the whole gamut full stack of web development I have a set of belief that I’ve come to developed toward technology stack. I didn’t fully express my view when talking to him and I’d like to put it down in words next time I have to discuss such topic.\nThis is also for me to have a clear, rational, and logical argument written down.\nKeeping Up To Date Technology is always changing. There is a new frontend rendering every few months. The people on the otherside will say, “It is up to you to learn and keep up to date.” I used to believe this until the realized this:\nIt sounds great but the fact is when your whole stack change often enough especially when you are a full stack this become a full time job.\nFrontend rendering rate of change is really really fast. That’s great if you don’t have no life, no ambition for relationship or family, etc…\nThe pro to frontend rendering is that we have tons of awesome stuff created by javascript community and I would argue that web component standardization was because of the cuttroat nature of flavor flav of the month frontend rendering javascript.\nI am done with this rat race.\nReason being I have other personal stuff to work on. My goals have changed from making tools for other people, to making tools for me and hoping that other people will find it useful. I’m doing side projects for myself. I get to choose what I feel like reasonable regardless if it’s wrong or not. I’ll learn from my mistakes.\nI am in the current mind set of learning something good and well enough that will survive in at least 2+ years from now.\nKeeping up to date is always something you hear but it’s importance does not overshadow having a reliable technology.\nLet’s start with Database SQL vs NoSQL One statement I made with my buddy is that I’m going to relearn and sharpen my relational database skillset and I’ve chosen PostgreSQL. I’ve been using the ORM libaries of MVC frameworks which abstract from the relation database. This is great but I want to be better and more focus on a piece of technology that I believe in. Plus I’m hoping to learn PostGIS in the future for temporal spatial analysis.\nHe wanted me to use Firebase instead of PostgreSQL.\nI don’t want to do this.\nOne of the biggest reason is that I want to keep my data safe and I want to be in control of my data. The application logic is not the end goal. The real gold, pun intended, is the data. It takes awhile to become an authorative site and developed an active community. The data is what makes this possible. Do not give your data to third party that’s silly.\nAnother reason is that relational database will be here in 10 years from now. I don’t know about anything else but you can try to guess and bet on it. I won’t be taking that bet or joining that rat race.\nI learned and did professionally Cassandra and MongoDB. I don’t know when and where it’ll be in a decade from now. I don’t believe it is a use case for the majority of the data out there, at least the data I care about. I’m not going to write joins manually. How well does my knowledge of column based database structure transferable to other databases? I think SQL is much more transferable than CQL.\nWhen you find yourself doing relational joins and doing nothing with text search in Elasticsearch, Solr, or any Lucene-based database then you’re doing it wrong.\nI can go into B+ trees vs trie but I get paid for that when I did consulting and it’s a huge discussion in itself.\nIt should be the responsibility of the programmer to do a pro and con. Often time than not, those programmers are chasing hype and money. That is fine. I don’t want to be the person getting stuck maintaining such choices when it goes out of vogue. Inheriting technical debt is not fun and if I have the choice then I choose no. Go out there and test those fancy new technology. I’ll wait until they are battle tested, I’ll wait until it’s boring like Ruby on Rails is now.\nThe majority of the data out there are relational. Use a relational database and if you want to gamble try one of the graph database. Also the big data hype is dying down. The majority of the data out there are not big. You don’t need fancy convoluted tech stacks for your small data. If you want to juggle and maintain convoluted tech stacks all day then sure.\nBack to Frontend Rendering Javascript Framework Another technology that keeps on changing is frontend rendering javascript frameworks. I am basically done with this, I went from ember.js, angular.js, and now to Vue.js. I’ll come back when it’s boring. I’ll come back when there are clear winners.\nI’ve highlight the reasons above. Other reason such as the sketchy security issue of npm packages. Or SEO for frontend rendering sucks, it is a bunch of hacks. Supposely Google got something that can handle crawling that. No thanks, I don’t need more headaches. SSR for me.\nI’ve chosen a server side/backend rendering MVC framework, Phoenix and Elixir programming language and call it a day. It’s a small community, the changes aren’t often, and more importantly it’s boring. I am pretty sure it’ll be here at least 2 years from now. It is a bet on my part on this technology but SSR aren’t changing as fast as FSR.\nI also dislike the Node.JS concurrecy model. I prefer Erlang’s actor model. It’s easier to think and debug. I don’t have to do hand threads myself or catch possible error cases. I can just let it crash and only catch cases that I want in Erlang.\nOS As for OS, I’m going for OpenBSD. I want to master an OS and I find that linux is increasingly complicated.\nWhich is fine but I want to know the in’s and out’s of my OS and OpenBSD is boring enough for me. It is small enough for me to figure out all the processes and what the hell is going on.I can tailor it to my developing machine without unnecessary bloat and blobs (heh). OpenBSD also have all the programming languages support I care about (Elixir, NPM,Python, R). I wish it have a better filesystem but I know it’ll work years from now. I’ll wait for Hammer2 and dream about a port over to OpenBSD one day.\nI have stopped doing rat races and refocus on what I care about and this is my new technology stack for my side projects:\nOpenBSD\rPostgreSQL\rPhoenix + Elixir\rThis is my rationality and hopefully there are some wisdoms in these.\nEtc… First picture: https://pixabay.com/en/users/RobinHiggins-1321953\rLast picture: https://pixabay.com/en/grimace-girl-teen-mimicry-brutal-1012862/\r","date":"2018-11-26T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/the-case-for-boring-technology/bored-3082828_1920_hua3f227cc1dad48243028196a49fdfc22_223249_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/the-case-for-boring-technology/","title":"The Case for Boring Technology"},{"content":"Update Busy moving. I have a summer project for this blog (look forward to it, it’ll be fun). I also a summer internship at JPL NASA!\nYou can get the pdf paper here.\nI just finished my semester and I did a final project that I’m pretty proud of. I put in a lot of effort and my professor Dr. Zhou was very awesome.\nAbstract Human immunodeficiency virus or HIV are responsible for decline in CD4+ cell count. The investigation is set out to find the population rate of CD4+ cell count decline per milliliter of blood, to characterize the of individual rate of cell decline, and the factors that predict cell decline. Using exploratory data analysis and longitudinal tools, a linear mixed effects model with random intercept and random slope was created. The estimated population average time course of CD4+ cell depletion is 80.1857 CD4+ cells per milliliter of blood. The degree of heterogeneity across men in the rate of progression as time passes is 54.8061127978 cell count. The factors that predict cell count decline is time, pack of smoke, number of sexual partners, cesd mental illness score, age \u0026amp; time interaction, and smoke \u0026amp; time. The time factor is the most dramatic in term of CD4+ cell depletion.\n1 Introduction 1.1 HIV and CD4+ Cells Human immunodeficiency virus or HIV is a virus that attack immune system by killing a class of immune cell named CD4+ cell. On average a normal person without HIV have 1000 cells per milliliter of blood. As time passes from the initial HIV infection an infected person CD4+ cell counts starts to decline. Acquired immune deficiency syndrome or AIDS is the disease caused by the HIV virus.\n1.2 The Data The data used in this paper is a subset of the Multicenter AIDS Cohort Study with 369 men with HIV. The data consist of columns representing: time since seroconversion, CD4 count, age (relative to arbitrary origin), packs of cigarettes smoked per day, recreational drug use (yes/no), number of sexual partners, CESD (mental illness score), and subject ID. The data have been standardized, the measurements are unbalance, and the time interval are not evenly spaced.\n1.3 Aim of the Investigation The aim of the investigation is four main points: average time course of CD4+ cell depletion, time course for individual men, to characterize the degree of heterogeneity across men in the rate of progression, and factors which predict CD4+ cell changes.\n2 Methods 2.1 Exploratory Data Analysis The goal in exploratory data analysis (EDA) is to have an idea what the CD4+ cell count data looks like and ideas to go from EDA to modeling the data. Creating a response trend model will give an idea how time affect the response and if polynomial time is needed. A variogram graph will indicate what kind of variance is needed to be account for in the model. There are three different kind of variance either random effect variance, within-subject variance, and between-subject variance are needed.\n2.2 Modeling Longitudinal Data The next step is to create a suitable longitudinal model for the CD4+ cell data to answer the aim of this investigation. The model that will be chosen will have to address the variances that was shown in the variogram during EDA. After the model is selected the next step will be predictor selection. The predictor selection will be base on the deviance test of the full and the reduced model. Deviance test will be perform because the comparison are base on nested models.\n2.3 Assumptions The assumptions this investigation made is there are between-subject variations, within-subject variations, and measurement variations that need to be explicitly accounted for. The chosen longitudinal model will account for these explicitly so that the investigation can have an accurate and precise answers to the aim of this investigation.\nBetween-subject is latent factors. Latent factors are biological variability examples are diet, genetics, and other latent factors. Latent factors can keep an individuals CD4+ cell count consistently higher than the population mean or lower than the population mean.\nThe within-subject variation is serial correlation. The serial correlation is induced by time, the close two measurements are the more correlated they are. The farther apart two measurements are the less correlated they are.\nMeasurement variation takes into account for the process of taking measurements is an imperfect process and that there will be some variation in taking CD4+ cell count measurement. A variogram with force equally spacing of time intervals will confirm these assumptions of variations exist in the CD4+ cell count data.\n3 Results 3.1 Exploratory Data Analysis Results Figure 1: A graph between the response of the CD4+ cell count on the y-axis and the time points on the x-axis.\nThe spaghetti plot, Figure 1, shows that the data is unbalanced and that the time intervals are irregular and unequaled. It also show that individual have different base line which imply random intercept and that individual have different rate of progression which imply random slope. This will help in model selection especially when certain covariance structure have assumption about balance data and equally spaced time intervals.\nFigure 2: A graph between the response of the CD4+ cell count on the y-axis and the time points on the x-axis.\nThe response trend graph, Figure 2, indicate that perhaps time is not constant but some sort of polynomial. Between time point 0 and 2 months there is a sharp drop in CD4+ cell count and closer to the 2 month time point the CD4+ cell count rate of decline starts to steady out and the sharp decrease rate is slowed down drastically. Modeling the data with quadratic or cubic time predictor may be needed base on this graph.\nFigure 3: A variogram of the CD4+ cell count data with time intervals forced to be equally space.\nNext is a plotted variogram (Figure 3) to check the assumption of having three sources of variation. Due to the data having unequaled time intervals the measurements are averaged and binned to the nearest time point. The blue line represent that variogram line and the grey horizontal line represents total variance.\nLooking at Figure 3, the variogram blue solid line does not start at zero it indicate that there exist measurement errors. The variogram is not a flat blue line but a slanted line with a slope indicating that there exist serial correlation. Finally the blue line does not touched the upper limit of total variance indicating that there is random effect in play. The assumption that the CD4+ cell count data have all three sources of variation can be safely assume and is verified empirically.\n3.2 Model Selection and Rejected Models Longitudinal analysis have many linear models that to choose from. Models such as unstructured covariance and structured covariance. This section will discuss the reason for not choosing certain models.\nUnstructured covariance is ruled out for two reasons. The first reason being that the large data set and large number of predictors would result in a large amount of parameter estimations. The second reason is that unstructured covariance is unsuitable for data set that have measurement taken at unequally spaced intervals.\nToeplitz covariance structure and autoregressive covariance structure both are other choices of structured covariance model. Both toeplitz and autoregressive assume that measurements are made at equal intervals of time. The CD4+ cell data have irregular unequal intervals of time.\nThe variogram shows there are three sources of variation. Independent model is rejected because the model assume there is only measurement error. Uniform model is also rejected because it only address two sources of variation, measurement error and between-individual variation. Exponential covariance model is rejected because the model address only within-individual variation.\nLinear mixed effects models is chosen is because the model addresses all three sources of variation. The model explicitly distinguished between fixed and random effects. The advantage of this explicit distinction enable accurate and precise answers to the aim of this investigation.\n3.3 Predictor Selection Predictors β_hat values p-values for t-test\rintercept\r790.11\r\u0026lt;.0001\rtime\r-81.6092\r\u0026lt;.0001\rage\r1.6277\r0.3790\rsmoke\r41.0459\r\u0026lt;.0001\rdrug\r22.6537\r0.2677\rpartners\r6.5509\r0.0043\rcesd\r-2.3499\r0.0070\rage × time\r-1.3805\r0.0317\rsmoke × time\r-14.2323\r\u0026lt;.0001\rdrug × time\r-1.7315\r0.8488\rpartners × time\r-0.3958\r0.7161\rcesd × time\r0.1585\r0.6899\rtime^2\r0.8753\r0.6187\rTable 1: Full linear mixed effects model estimate.\nAfter choosing the linear fixed effects model with random intercept and random slope to model the data, the next part is selecting a good combination of predictors that describe the CD4+ cell count data. A full model is fitted first. From Table 1, which show the estimated β, predictors that are not significant at p-value of 0.05 will be dropped and the predictors that are significant will be kept as a reduced model. Note the time^2 was included in the full model because of the nonlinear trend of time that was indicated in the response trend graph.\nThe predictors that are dropped are drug, drug × time, partners × time, cesd × time, and time^2. Even though the age predictor is not significant the interaction age × time is significant therefore the age predictor is kept in the reduced model.\nFull Model Reduced Model\rintercept\r790.11\r\u0026lt;.0001\r-2 Log Likelihood\r33603.4\r33600.9\rχ2 Test\r2.5\r2.5\rStatistic Degree of Freedom\r13\r8\rχ2 25,0.95\r11.070\r11.070\rTable 2: Likelihood Ratio test for two linear mixed effect models.\nHypothesis H1: Reduced Linear Mixed Effects Model Hypothesis H2: Full Linear Mixed Effects Model\nAfter fitting the reduced model, a likelihood ratio test was conducted between the full model and the reduced model. Table 2 shows the χ2 test statistic at 2.5 which is the difference between the -2 Log Likelihood of full model and reduced model. The degree of freedom for χ2 is the difference between the number of parameters in the full model and the number of parameters in the reduced model which is 5. The null hypothesis for the deviance test is the reduced model and the alternative hypothesis is the full model. Since the test statistic is 2.5 which is much less than 11.070, the reduced model is chosen.\n3.4 Final Model The equation listed below is the selected model that best represent the CD4+ cell count data and the best explanation of the data. With this model, the investigation can proceed to answer the aim of the investigation.\nYij = β0 + β1 timeij + β2 ageij + β3 smokeij + β4 partnersij + β5 cesdij + β6 ageij × timeij + β7 smokeij × timeij + b0i + b1i × timeij + eij\r=791.05 − 80.1857timeij + 1.4697ageij + 38.0785smokeij + 7.0434partnersij − 2.2867 cesdij − 1.3400 ageij × timeij − 13.2674 smokeij × timeij + b0i + b1i timeij + eij (1)\rWhere b0i represents the random intercept for each individual and b1i represents the random slope for each individual.\nThe model can be rewritten in matrix notation\nYi=Xiβ+Zibi +ei, i=1,…,N,j=1,…,ni (2) where Y i is a vector of size ni × 1 representing observations for ith individual, j represent the jth measurement for ith individual, Xi is a ni × p design matrix of p independent fixed effect variables, Zi is a ni × q design matrix of q independent random effect variables, β is a vector of size p × 1 representing fixed effect parameters, bi is an independent vector of q × 1 size representing random effects with MVN(0,G) distribution (Multivariate Normal), and ei represents an independent vector of random errors of size ni ×1 with MVN(0,Ri) distribution. The ei are independent of bi.\nThe Ri represent within-subject variance. Linear fixed effects model break Ri down into two sources of within-subject variance, serial correlation and measurement error. The measurement error variance (τ^2) is equal to 59104. The serial correlation variance (σ^2) is 1.0649. The G matrix represents the between-subject variance.\nSee paper for more matrix notations… \u0026gt;__\u0026lt;\nEtc… Please see paper for results, SAS codes, R codes, and conclusion. The blog post is getting long.\nPost Mordem Well… translating a paper into a blog post is terrible. The paper is too academic with high domain assumption and an abstract and a link to the paper is sufficient.\nEtc.. The Freddie Mercury picture is taken from pixabay.","date":"2018-06-07T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/linear-mixed-effects-models-for-cd4-cell-counts-in-men-with-hiv/freddie-mercury-memorial-779956_640_hu53777eb8fcd46320722b3f01d82c23b1_63523_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/linear-mixed-effects-models-for-cd4-cell-counts-in-men-with-hiv/","title":"Linear Mixed Effects Models for CD4+ Cell Counts in Men with HIV"},{"content":"Introduction I’m into light novels so I’ve decided to combine my love for data science and light novel.\nInformation Retrieval I started out by retrieving some data from online via scraping using scrapy. I love scrapy and highly recommend it for any serious web scrapping task.\nThe Data So what does the data look like?\n2739 novels.\nIt’s in a json format.\nThe columns are novel title, description, novel type, genre, tags, rating, language, authors, year, and license.\nExploratory Data Analysis For this portion I will be using R to do some EDA (❤ John Tukey).\nThere are eight languages in the data including 7 novels with no language category.\nThe languages are: Japanese, Chinese, Korean, Malaysian, Filipino, Indonesian, Thai, and Vietnamese.\nLanguages Number of Novels Average Ratings Japanese 1496 3.994248 Chinese 1059 3.978571 Korean 131 4.142424 Filipino 23 NA Indonesian 12 NA Thai 7 NA Malaysian 3 NA Vietnamese 1 NA Uncategorized 7 NA The average ratings only include novels that have at least 30 user ratings.\nI did not take any other rating from other countries because there are less than 30 novels. As a statistician I feel 30 or more is a sufficient number to represent the population for each category anything smaller than is insufficient.\nI plot a histogram plot for all novels from all countries with 30 or more user ratings. It is left skewed. While I can only speculate that people tend to rate for their favorite novels. It’s interesting because there is quite a lot of quality novels out there and many hover above 4.0 rating out of 5.0.\nThere is a small number with perfect five ratings. 19 novels with 5 ratings all of them have single digit number of users reviews except for one (“Mum, I Used to Hate You”). It have 18 users that reviewed it with a 5 rating.\nSweet let’s find novels that have high rating and at least 30 user reviews! I tried 4.5 at first but it’s a huge list at least 300+ so I decided to look for 4.7 ratings. There is exactly 75 novels with that criteria.\nHere’s the list and hope you guys find it useful.\nA Game To Make Him Fall\rA Will Eternal\rA Slight Smile is Very Charming\rAme no Hi no Iris\rAt the Northern Fort\rBy A Slight Mistake\rCultivation Chat Group\rClockwork Planet\rDemon Girl ~Tale of a Lax Demon~\rDungeon Defense\rEight Treasures Trousseau\rGekkou\rGolden Age Legitimate Fei\rHokuou Kizoku to Moukinzuma no Yukiguni Karigurashi\rHiraheishi wa Kako o Yumemiru\rHikaru ga Chikyuu ni Itakoro……\rI Reincarnated into an Otome Game as a Villainess With Only Destruction Flags…\rI Don’t Like The World, I Only Like You\rIt’s Because You Said There Would Be Candy!!\rJoy of Life\rKenkyo, Kenjitsu o Motto ni Ikite Orimasu\rKaze no Stigma\rKono Kamen no Akuma ni Sodan wo!\rKuzu to Kinka no Qualidea\rManuscript Screening Boy and Manuscript Submitting Girl\rMarietta-hime no Konrei\rMarginal Operation\rMaoyuu Maou Yuusha\rMimizuku to Yoru no Ou\rMondaiji-tachi ga Isekai kara Kuru Sou Desu yo?\rMulberry Song\rMy Death Flags Show No Sign of Ending\rNo Game No Life\rNobunaga’s Imouto is My Wife\rOokami to Koushinryou\rOverlord (LN)\rOuroboros Record ~Circus of Oubeniel~\rOur Second Master\rQuickly Wear the Face of the Devil\rRakuin no Monshou\rRain\rRelease that Witch\rRunning Away From The Hero!\rSansheng, Wangchuan Wu Shang\rSemi Datte Tensei Sureba Ryuu Ni Naru\rSweet Heart in Honeyed Desire\rTabi ni Deyou, Horobiyuku Sekai no Hate Made\rThe Bathroom Goddess\rThe Destruction of a Triad Boss Trilogy\rThe Girl Who Bore the Flame Ring\rThe Girl Who Ate a Death God\rThe Founder of Diabolism\rThe Legend of Sun Knight\rThe Grandmaster Strategist\rThe Magnificent Battle Records of A Former Noble Lady\rThe Princess Wei Yang\rThe Probability I Can Kill My Wife Without Being Found Out\rThe Other World Dining Hall (LN)\rThe Rebirth of the Malicious Empress of Military Lineage\rThe Scum Villain’s Self-Saving System\rThe Tang Dynasty’s Female Forensic Doctor\rThe Witch and the Gourd of Stories\rTo Be A Virtuous Wife\rThree Days of Happiness\rUchi no Musume no Tame naraba, Ore wa Moshikashitara Maou mo Taoseru kamo Shirenai (LN)\rUchi no Musume no Tame naraba, Ore wa Moshikashitara Maou mo Taoseru kamo Shirenai (WN)\rTsuyokute New Saga (LN)\rVermillion\rUtsuro no Hako to Zero no Maria\rWhy Is the Prettiest Girl in School Trying to Talk to a Loner Like Me during Lunch Break?\rWhen He Comes, Close Your Eyes\rVirtual World: Peerless White Emperor\rWoof Woof Story ~ I Told You I am a Rich Person’s Dog, Not Fenrir ~\rYahari Ore no Seishun Love Come wa Machigatte Iru\rYoujo Senki\rConclusion I think I’ll be doing more light novel analysis in the future.\nI think I’ll do NLP too something like sentiment analysis or something for those tags and categories. Of course this is when I have time.\nWhat Did I Get to Practice? (for me) My data science \u0026amp; statistic skills using R. My web scraping skill using scrapy. My devops skill, I made a custom vagrant for jekyll and used ansible to self provision. I created a new blog just for this post. I made a yummy pie chart. Etc.. The novel picture is taken from google searching for a CC license image.\n","date":"2017-07-17T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/light-novel-analysis-part-1/lightnovel_hue3f6c131a135f7a211aab575893b8d77_402324_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/light-novel-analysis-part-1/","title":"Light Novel Analysis (part 1)"},{"content":"Introduction So I’ve learned a little bit about Bayesian Hierarchical Modeling at FDA and decided to put down my thoughts and write about it more to reinforced what I’ve learned. I also want to try out some new javascript data visual libraries.\nA great book I’ve found is, “Introduction to Hierarchical Bayesian Modeling for Ecological Data” by Parent and Rivot [1].\nWhile at the FDA I code my own model without using any MCMC framework and it was very slow in R. I realize I need a MCMC framework under my toolbelt. After some research I decided on Stan using the rstan r package.\nThe Graph (not DAG; not a Bayesian network) Graph represents the salmon migration and birth cycle. Each edge represent a year pass. The nodes are square because they’re given. The information is given from previous knowledge.\nVariable\rDefinition\rWt\rSalmon Eggs\r0+\rYoung-of-the-year (hatched)\rPSm\rPre-smolts\rSm1\rSmolt after 1 year\rSp1\rReturns one year earlier than Sp2\rParr1\rSmaller juveniles left behind by Sm1\rSm2\rSmolt after 2 year\rSp2\rReturns one year after Sp1\rThe Models - Introducing Probability into the Graph\rWe’re going to take the graph that represent Salmon’s migration cycle and introduce uncertainty to it (model it via probability). By doing this we create a new graph that is complete different from the Salmon’s migration cycle graph. It is a graph base on probability view.\rWe go through each square node and one by one apply a model and probability to it.\rModel is time base, t will represent a particular year.\rSp t = Sp1 t + Sp2 t = # of spawners at t-th year\rW t = # of eggs spawned by the adults returning in year t\r0+ t = Young-of-the-year at t-th year\rPSm t = pre-smolts at t-th year\rSm1 t = 1+ smolts (1 year to smolt) at t-th year\rP1 t = Parr1 = smaller juveniles left behind by Sm1 at t-th year\rSm2 t = 2+ smolts (2 years to smolt) at t-th year\rSpawners -\u0026gt; Eggs\rW t = Sp t ⋅ P_f ⋅ fec\rW t = # of eggs spawned by the adults returning in year t\rSp t = # of spawners = Sp1 + Sp2\rP_f = proportion of females\rfec = mean of fecundity (fertility)\rEggs -\u0026gt; 0+ juveniles\rThis is Ricker Cruve model with parameters (α,β) which is a classic discrete population model.\r0+t+1 = α ⋅ Wt ⋅ e-β ⋅ Wt ⋅ eεt where εt ~iid N(0, σ2)\r0+t+1 = freshwater production of juveniles resulting from the reproduction of the spawners returning in year t 0+ juveniles -\u0026gt; Smolts\nPSmt+2 ~ Binomial(0+t+1, γ0+) = # of 0+t+1 will survive and migrate to PSmt+2\nSm1t+2 ~ Binomial(PSmt+2, θSm1) = # of PSmt+2 will survive and migrate as 1+smolts (Sm1)\nSm2t+3 ~ Binomial(Parr1t+2, γparr1) = # of Parr1t+2 will survive and migrate as 2+smolts (Sm2)\nPSmt+2 = young-of-the-year 0+t+1 will survive next spring year t+2γ0+ = survival rate of 0+θSm1 = proportion of pre-smolts will migrate as 1+Smolts (survival rate)γparr1 = survival rate of parr1\rSp1t+3 ~ Binomial(Sm1t+2, γSm)\nSp2t+4 ~ Binomial(Sm2t+3, γSm)\nγSm = survival rate\rLearning from observations\nThese two are observed and given:\nCSm1,t = observations = # of smolts caught downstream trap\nπSm = trap efficiency\nUsing these data points we can figure out the unknowns.\nOur unknowns, the parameters, are: α, β, σ, γ0+, θsm1, γParr1, γSm\n# of smolts caught downstream trap can be model as a binomial distribution either the smolt is caught or not.\nCSm1,t ~ Binomial(Sm1t, πSm)\n*Note (advance): observations assume Bayesian’s property of exchangability\nThe Models - Creating a probability graphical model\r0+t+1 = α ⋅ Wt ⋅ e-β ⋅ Wt ⋅ eεt where εt ~iid N(0, σ2)\n0+t+1 = freshwater production of juveniles resulting from the reproduction of the spawners returning in year t\rPSmt+2 ~ Binomial(0+t+1, γ0+) = # of 0+t+1 will survive and migrate to PSmt+2\nγ0+ = survival rate of 0+\rSm1t+2 ~ Binomial(PSmt+2, θSm1) = # of PSmt+2 will survive and migrate as 1+smolts (Sm1)\nPSmt+2 = young-of-the-year 0+t+1 will survive next spring year t+2θSm1 = proportion of pre-smolts will migrate as 1+Smolts (survival rate)\rSm2t+3 ~ Binomial(Parr1t+2, γparr1) = # of Parr1t+2 will survive and migrate as 2+smolts (Sm2)\nSp1t+3 ~ Binomial(Sm1t+2, γSm)\nSp2t+4 ~ Binomial(Sm2t+3, γSm)\nNow we put all the parts together into graph. Okay, now that we got the probability graphical model down we can figure out the joint probability distribution.\nP(Jt) = ?\nStep 1. Looking at the graph, we’re going to start with all nodes with no parent: α, β, σ, Wt, γ0+, θSm1, γParr1, and γSm.\nP(Jt) = P[α] ⋅ P[β] ⋅ P[σ] ⋅ P[Wt] ⋅ P[γ0+] ⋅ P[θSm1] ⋅ P[γParr1] ⋅ P[γSm] … Step 2. Now we’re going to look at the nodes with parents.\n0+t+1 is P[0+t+1 | Wt, α, β, σ]PSmt+2 is P[PSmt+2 | 0+t+2, γ0+]Sm1t+2 \u0026amp; Parr1t+2 is P[Sm1t+2, Parr1t+2 | PSmt+2, θSm1]. Notice how complex this one is. It is because Sm1 and Parr1 both share the same parameters.P[Sp1t+3 | Sm1t+2, γSm]P[Sm2t+3 | Parr1t+2, γParr1]P[Sp2t+4 | Sm2t+3, γSm]\rP(Jt) = P[α] ⋅ P[β] ⋅ P[σ] ⋅ P[Wt] ⋅ P[γ0+] ⋅ P[θSm1] ⋅ P[γParr1] ⋅ P[γSm] \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; \u0026nbsp; ⋅ P[0+t+1 | Wt, α, β, σ] ⋅ P[PSmt+2 | 0+t+2, γ0+] ⋅ P[Sm1t+2, Parr1t+2 | PSmt+2, θSm1]\r\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp; ⋅ P[Sp1t+3 | Sm1t+2, γSm] ⋅ P[Sm2t+3 | Parr1t+2, γParr1] ⋅ P[Sp2t+4 | Sm2t+3, γSm] Okay so what? Where’s the Bayesian network?\rNot yet. The book needs to introduce the concept of a simple model vs a hierarchical model and some terminology.\nSo far we haven’t introduce any observational variable (random variable) at all.\nθ represents parametersZ represents latent parametersY represents the output Random Variable. (little y represent the realization/sample of Y random variable).\rLeft is a simple model. The right graph is a hierarchical model.\nZ represents latent variables (nuisance variables), basically variables we don’t really care, also they’re hidden we don’t observed it directly like Y. Y represents observations. Observations are random so Y is capitalized and smaller y is the realization of Y or a sample of Y. The node is pink because it is an observable.\nP[θ, Z, Y] = P[θ] ⋅ P[Z | θ] ⋅ P[Y | θ, Z]\nWell first what’s the experiment?\nFor this it’s salmon captures and they’re model via binomial distribution either you catch the fish or not.\nNotice the C stands for catches.\nC0+, t+1 ~ Binomial(0+t+1, π0+)CSm1, t+2 ~ Binomial(Sm1t+2, πSm)CSm2, t+3 ~ Binomial(Sm1t+3, πSm)CSp1, t+3 ~ Binomial(Sp1t+3, πSp)CSp2, t+4 ~ Binomial(Sp2t+4, πSp)\rOnce again the squares represent known/given values (the π’s are given). The pink circle means observed values. Pink in general means they’re known either by given or by observations. The purple boxes represent grouping and group the nodes into their respective group.\nOk. Finally, we got a Bayesian network. Really, what now?\nHow does y (the sample or realization of Y) fits in this fancy graph?\nWhat happen when the Observation is available?\rBefore that notice how we build the model and the direction. The direction is downward from the Salmon cycle toward the latent variable and then towards the obsevation.\nWhy did the book brought this up? It is because when you train the model using the data/observations that are available you go in the opposite direction.\nYou start at the Y (observation layer and Y is a random variable) and Y is now, Y = y, since little y is the realization of random variable Y. y is a sample of Y or the data (values not just some placeholder variable). And you go up to latent layer and then to the parameter later.\nLet’s see it mathematically:\nHere’s the joint probability:\nP[Y, θ, Z]\nNow here’s the joint probability with Y = y, when we have data to train the model and find the paramenter.\nP[θ, Z | Y = y]\nGiven Y = y, the observations propagate upward from the observation to the latent layer to the parameter layer.\nThis is how you train the model after you are done creating the model.\nYou can see the Bayes Rule connection too right? We’re always dealing with Joint Probability and Conditional Probability.\nBayesian make it so that they’re conditionally independent. This is one of the property of Bayesian statistic.\nThis is now a posterior distribution. Posterior being after the data. Prior distribution is before the data.\nP[θ, Z | Y = y] = posterior distributionundefined\rI’m going to repeat it again.\nPosterior is after the data have been inputed.\nPrior is before the data. It is your prior belief.\nIn Bayesian you need to supply a belief in form of a prior distribution. It’s weird but don’t worry if you don’t know anything then you can use a noninformative prior distribution.\nThe belief thing is also away to encode expert belief too.\nSo given what we have now, we just have to apply Bayes’ Rule to the conditional probability and you get your parameter values.\nBayes’ Rule\rP[θ Z | Y = y] = P[θ, Z, Y = y] / P[Y = y]\nSome stat here and you get.\nP[θ Z | Y = y] ∝ P[θ] ⋅ P[Z | θ] ⋅ P[Y = y | θ, Z]\nConclusion\rI highly recommend this book. Andrew Gelman’s DBA book is more PhD level and his approach is not graphical like this but more mathy. Being visual this book helps a lot into tying things together.\nThere was no observations/data and no code for this chapter. Ah dangit. Well until next time, stay tuned for the next episode of Bayesian man.\n1Buy the book if you like what you see on the post. This is basically my notes on chapter 1 of the book. It’s an amazing book and I highly recommend it.\nWhat did I learn about myself\nI’m glad I’m reviewing this chapter of the book again. I have a confession to make, if I want to understand a material/subject I need to read 3 times and do projects on it and a review of what I’ve learned. I need tons of practice. I guess this is one of the reason why I started this blog.\nThis chapter ties in again DAG, Bayes’ Rule, and conditional probabilities. Good refresher and clear up things that I was wrong about. Especially the salmon breeding cycle, I didn’t think about the fact that it wasn’t a DAG. And that from that model we create a Bayesian Graph Model (DAG).\nI think I’ll go through each chapter of this book as a refresher while playing with javascript graphical libraries and hopefully learn Stan. I need to make sure I didn’t miss out on anything from the first reading.\nWhat Did I Get to Practice? (for me)\nBayesian Hierarchical Modeling using rstan.Tried out a javascript data visualisation library, cytoscape.js, for modeling graphs.Gets to refresh Bayesian Graphical Model (Bayesian Network).\rRough Roadmap for Bayesian HM\nFinish off this book. Introduction to Hierarchical Bayesian Modeling for Ecological Data (Chapman \u0026amp; Hall/CRC Applied Environmental Statistics)Read this for Hamiltonian Markov Chain(Statistics in the social and behavioral sciences series) Gill, Jeff-Bayesian Methods A Social and Behavioral Sciences Approach-CRC Press (2014)Read https://arxiv.org/abs/1111.4246 an implementation of HMCMeasure theory videosDBA 3 reread again learning Dirichlet Process\rEtc..\rIntroduction to Hierarchical Bayesian Modeling for Ecological Data (Chapman \u0026amp; Hall/CRC Applied Environmental Statistics) The book link is Amazon affiliated. If you get it at CRC publishing you can get it 20 bucks cheaper if you use a discount code, just that it takes longer to ship. Also note I would recommend reading “Doing Bayesian Data Analysis” first before even trying to get into Hierarchical Modeling.Would like to thank this website for all the html mathematical notations.\rThe salmon sushi picture was taken from pixabay under creative common license.\r","date":"2017-07-15T00:00:00Z","image":"https://mythicalprogrammer.github.io/p/book-note-intro-to-hierarchical-bayes-modeling-eco-data-chp-one/salmon_sushi_hu4711793190d15acc5555df5b859ceb84_343879_120x120_fill_q75_box_smart1.jpg","permalink":"https://mythicalprogrammer.github.io/p/book-note-intro-to-hierarchical-bayes-modeling-eco-data-chp-one/","title":"Book Note: Introduction to Hierarchical Bayesian Modeling for Ecological Data, Chapter 1"}]